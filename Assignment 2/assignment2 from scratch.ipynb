{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "training_dataset = [\n",
    "        [0.49, 0.29, 0.56, 0.24, 0.35, 0.0],\n",
    "        [0.07, 0.4, 0.54, 0.35, 0.44, 0.0],\n",
    "        [0.56, 0.4, 0.49, 0.37, 0.46, 0.0],\n",
    "        [0.59, 0.49, 0.52, 0.45, 0.36, 0.0],\n",
    "        [0.23, 0.32, 0.55, 0.25, 0.35, 0.0],\n",
    "\n",
    "        [0.67, 0.39, 0.36, 0.38, 0.38, 0.0],\n",
    "        [0.29, 0.28, 0.44, 0.23, 0.34, 0.0],\n",
    "        [0.21, 0.34, 0.51, 0.28, 0.39, 0.0],\n",
    "        [0.2, 0.44, 0.46, 0.51, 0.57, 0.0],\n",
    "        [0.42, 0.4, 0.56, 0.18, 0.3, 0.0],\n",
    "    \n",
    "        [0.42, 0.24, 0.57, 0.27, 0.37, 0.0],\n",
    "        [0.25, 0.48, 0.44, 0.17, 0.29, 0.0],\n",
    "        [0.39, 0.32, 0.46, 0.24, 0.35, 0.0],\n",
    "        [0.51, 0.5, 0.46, 0.32, 0.35, 0.0],\n",
    "        [0.22, 0.43, 0.48, 0.16, 0.28, 0.0],\n",
    "\n",
    "        [0.25, 0.4, 0.46, 0.44, 0.52, 0.0],\n",
    "        [0.34, 0.45, 0.38, 0.24, 0.35, 0.0],\n",
    "        [0.44, 0.34, 0.55, 0.52, 0.58, 0.0],\n",
    "        [0.23, 0.4, 0.39, 0.28, 0.38, 0.0],\n",
    "        [0.41, 0.57, 0.39, 0.21, 0.32, 0.0],\n",
    "\n",
    "        [0.06, 0.61, 0.49, 0.92, 0.37, 1.0],\n",
    "        [0.44, 0.52, 0.43, 0.47, 0.54, 1.0],\n",
    "        [0.63, 0.47, 0.51, 0.82, 0.84, 1.0],\n",
    "        [0.23, 0.48, 0.59, 0.88, 0.89, 1.0],\n",
    "        [0.34, 0.49, 0.58, 0.85, 0.8, 1.0],\n",
    "    \n",
    "        [0.43, 0.4, 0.58, 0.75, 0.78, 1.0],\n",
    "        [0.46, 0.61, 0.48, 0.86, 0.87, 1.0],\n",
    "        [0.27, 0.35, 0.51, 0.77, 0.79, 1.0],\n",
    "        [0.52, 0.39, 0.65, 0.71, 0.73, 1.0],\n",
    "        [0.29, 0.47, 0.71, 0.65, 0.69, 1.0],\n",
    "    \n",
    "        [0.55, 0.47, 0.57, 0.78, 0.8, 1.0],\n",
    "        [0.12, 0.67, 0.74, 0.58, 0.63, 1.0],\n",
    "        [0.4, 0.5, 0.65, 0.82, 0.84, 1.0],\n",
    "        [0.73, 0.36, 0.53, 0.91, 0.92, 1.0],\n",
    "        [0.84, 0.44, 0.48, 0.71, 0.74, 1.0],\n",
    "    \n",
    "        [0.48, 0.45, 0.6, 0.78, 0.8, 1.0],\n",
    "        [0.54, 0.49, 0.4, 0.87, 0.88, 1.0],\n",
    "        [0.48, 0.41, 0.51, 0.9, 0.88, 1.0],\n",
    "        [0.5, 0.66, 0.31, 0.92, 0.92, 1.0],\n",
    "        [0.72, 0.46, 0.51, 0.66, 0.7, 1.0]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12, 0.67, 0.74, 0.58, 0.63, 1.0], [0.25, 0.4, 0.46, 0.44, 0.52, 0.0], [0.52, 0.39, 0.65, 0.71, 0.73, 1.0], [0.22, 0.43, 0.48, 0.16, 0.28, 0.0], [0.5, 0.66, 0.31, 0.92, 0.92, 1.0], [0.27, 0.35, 0.51, 0.77, 0.79, 1.0], [0.67, 0.39, 0.36, 0.38, 0.38, 0.0], [0.56, 0.4, 0.49, 0.37, 0.46, 0.0], [0.44, 0.52, 0.43, 0.47, 0.54, 1.0], [0.29, 0.47, 0.71, 0.65, 0.69, 1.0], [0.72, 0.46, 0.51, 0.66, 0.7, 1.0], [0.59, 0.49, 0.52, 0.45, 0.36, 0.0], [0.25, 0.48, 0.44, 0.17, 0.29, 0.0], [0.2, 0.44, 0.46, 0.51, 0.57, 0.0], [0.42, 0.4, 0.56, 0.18, 0.3, 0.0], [0.46, 0.61, 0.48, 0.86, 0.87, 1.0], [0.07, 0.4, 0.54, 0.35, 0.44, 0.0], [0.06, 0.61, 0.49, 0.92, 0.37, 1.0], [0.34, 0.45, 0.38, 0.24, 0.35, 0.0], [0.4, 0.5, 0.65, 0.82, 0.84, 1.0], [0.48, 0.41, 0.51, 0.9, 0.88, 1.0], [0.84, 0.44, 0.48, 0.71, 0.74, 1.0], [0.73, 0.36, 0.53, 0.91, 0.92, 1.0], [0.48, 0.45, 0.6, 0.78, 0.8, 1.0], [0.63, 0.47, 0.51, 0.82, 0.84, 1.0], [0.41, 0.57, 0.39, 0.21, 0.32, 0.0], [0.34, 0.49, 0.58, 0.85, 0.8, 1.0], [0.23, 0.48, 0.59, 0.88, 0.89, 1.0], [0.55, 0.47, 0.57, 0.78, 0.8, 1.0], [0.21, 0.34, 0.51, 0.28, 0.39, 0.0], [0.29, 0.28, 0.44, 0.23, 0.34, 0.0], [0.49, 0.29, 0.56, 0.24, 0.35, 0.0], [0.23, 0.32, 0.55, 0.25, 0.35, 0.0], [0.44, 0.34, 0.55, 0.52, 0.58, 0.0], [0.54, 0.49, 0.4, 0.87, 0.88, 1.0], [0.23, 0.4, 0.39, 0.28, 0.38, 0.0], [0.42, 0.24, 0.57, 0.27, 0.37, 0.0], [0.43, 0.4, 0.58, 0.75, 0.78, 1.0], [0.39, 0.32, 0.46, 0.24, 0.35, 0.0], [0.51, 0.5, 0.46, 0.32, 0.35, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "training_dataset = shuffle(training_dataset)\n",
    "print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12, 0.67, 0.74, 0.58, 0.63, 1.0], [0.25, 0.4, 0.46, 0.44, 0.52, 0.0], [0.52, 0.39, 0.65, 0.71, 0.73, 1.0], [0.22, 0.43, 0.48, 0.16, 0.28, 0.0], [0.5, 0.66, 0.31, 0.92, 0.92, 1.0], [0.27, 0.35, 0.51, 0.77, 0.79, 1.0], [0.67, 0.39, 0.36, 0.38, 0.38, 0.0], [0.56, 0.4, 0.49, 0.37, 0.46, 0.0], [0.44, 0.52, 0.43, 0.47, 0.54, 1.0], [0.29, 0.47, 0.71, 0.65, 0.69, 1.0], [0.72, 0.46, 0.51, 0.66, 0.7, 1.0], [0.59, 0.49, 0.52, 0.45, 0.36, 0.0], [0.25, 0.48, 0.44, 0.17, 0.29, 0.0], [0.2, 0.44, 0.46, 0.51, 0.57, 0.0], [0.42, 0.4, 0.56, 0.18, 0.3, 0.0], [0.46, 0.61, 0.48, 0.86, 0.87, 1.0], [0.07, 0.4, 0.54, 0.35, 0.44, 0.0], [0.06, 0.61, 0.49, 0.92, 0.37, 1.0], [0.34, 0.45, 0.38, 0.24, 0.35, 0.0], [0.4, 0.5, 0.65, 0.82, 0.84, 1.0], [0.48, 0.41, 0.51, 0.9, 0.88, 1.0], [0.84, 0.44, 0.48, 0.71, 0.74, 1.0], [0.73, 0.36, 0.53, 0.91, 0.92, 1.0], [0.48, 0.45, 0.6, 0.78, 0.8, 1.0], [0.63, 0.47, 0.51, 0.82, 0.84, 1.0], [0.41, 0.57, 0.39, 0.21, 0.32, 0.0], [0.34, 0.49, 0.58, 0.85, 0.8, 1.0], [0.23, 0.48, 0.59, 0.88, 0.89, 1.0], [0.55, 0.47, 0.57, 0.78, 0.8, 1.0], [0.21, 0.34, 0.51, 0.28, 0.39, 0.0], [0.29, 0.28, 0.44, 0.23, 0.34, 0.0], [0.49, 0.29, 0.56, 0.24, 0.35, 0.0], [0.23, 0.32, 0.55, 0.25, 0.35, 0.0], [0.44, 0.34, 0.55, 0.52, 0.58, 0.0], [0.54, 0.49, 0.4, 0.87, 0.88, 1.0], [0.23, 0.4, 0.39, 0.28, 0.38, 0.0], [0.42, 0.24, 0.57, 0.27, 0.37, 0.0], [0.43, 0.4, 0.58, 0.75, 0.78, 1.0], [0.39, 0.32, 0.46, 0.24, 0.35, 0.0], [0.51, 0.5, 0.46, 0.32, 0.35, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset)\n",
    "X = torch.Tensor([i[0:5] for i in training_dataset])\n",
    "Y = torch.Tensor([i[5] for i in training_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVfrH8c+TDkkIhIReAtJ7IFIEEdQVUFnsAooNRVdk1V27u6Lu6qqsDbChIqIIFlBxpVhXEBBI6C20UCIlkRZCCGnP748ZfGX5pZJJ7mTmeb9e88rMPffOPMfyzc2Zc88VVcUYY4zvCnC6AGOMMZXLgt4YY3ycBb0xxvg4C3pjjPFxFvTGGOPjLOiNMcbHeW3Qi8hUEUkTkQ0eer98EVnjfswtx3E3iMg692OpiHQtZr9pIpJS6DO6eaDmZ0Rkr4hkVvS9jDH+S7x1Hr2I9Acygemq2skD75epqhGl7LNLVePO2HYesFlVj4jIEOBJVe1VxLHTgP+o6mcVrbXQe/YGdgPbSqvdGGOK47Vn9Kq6CDhceJuInCMiC0QkSUQWi0i7Kqhjqaoecb/8BWhSnuNFJNz918lKEVktIsPK8dm/qOr+8nyeMcacyWuDvhhTgHGq2gN4AHi9HMeGiUiiiPwiIlec5eePBuaX0P6Me4jnZREJdW97HPhBVc8FBgITRCT8LD/fGGPKzWuHbgBEJA7XcEgnEYkA0oHkQruEqmp7EbkKeLqIt/hVVQe536uRqu4TkZbAD8BFqrpDRF4D+rr37wBscj//VFWfKVTLQFy/WPqp6qEiam0IHABCcP1C2qGqT4tIIhAG5Ll3jQYG4RqWKu6XxiWquq/Qe5c67GSMMcWpTkFfC0hW1YYeeN9pFDGeXtQYvXt7F+BzYIiqbi3D+w8AHlDVy0UkCRipqsmlHFbS+1nQG2POWrUZulHVDCBFRK4FEJciZ8CcSUTqnB5KEZEYXGfwm0o+6vdjmwFzgFElhbz7jB4REeAK4PRsoYXAOPd2RCS+LJ9rjDGe4rVBLyIzgWVAWxFJFZHRwA3AaBFZC2wEyvrFZnsg0X3cj8BzqlqmoAeeAOoCr7unTSYWqnGeiDRyv5whIuuB9UAM8E/39n8AwcA691TRf5TxcxGRF0QkFajp/mfwZFmPNcaY07x66MYYY0zFee0ZvTHGGM8IcrqAosTExGhcXJzTZRhjTLWRlJT0m6rGFtXmlUEfFxdHYmJi6TsaY4wBQER2F9dmQzfGGOPjLOiNMcbHWdAbY4yPs6A3xhgfZ0FvjDE+zoLeGGN8nAW9Mcb4OK+cR3+2Jn6/jUa1axDfrDYt6oYTECBOl2SMMY7zmaDPyStg6pIUjmblAlArLIh6tcJ+b69fK5T4pnWIb1abbk1rUzcitLi3MsYYn+IzQR8SFMCqv/2BHemZrN5zlNV7j5Jx0hX6irLncBZv/LSD/ALXIm7NomsS36w27RvWomFUGA2jatCqXgTR4SFOdsMYYzzOK1evTEhI0MpYAuFkTj7rUo+yZu9R9y+DIxzMOPV7e0hQALeeF8fdA1oRVTPY459vjDGVRUSSVDWhqDafOaMvixohgfRqWZdeLev+vu14di4HM7LZdzSbL9fsY8rincxauZd7BrZiVJ/mhAUHOlixMcZUnF+d0ZfFpn0ZPL9gCz9tTadx7Ro8MKgNw7o2ti92jTFeraQzepteeYYOjWrx/m09mXF7L+qEB3P/x2u5dOJi5qxK5VRevtPlGWNMudkZfQkKCpSv1u1j0g/b2Z6WSUxEKDf1ac6tfeOIDLMxfGOM9yjpjN6CvgxUlcXbfuO9JSn8mJxOdHgIf76wFSN7NSckyP4oMsY4r0JBLyJNgelAA6AAmKKqr56xz4O4btwNri942wOxqnpYRHYBx4F8IK+4QgrztqAvbH3qMZ6dt5llOw/RvG5NHhzUlss6N0TExvCNMc6paNA3BBqq6ioRiQSSgCtUdVMx+w8F7lfVC92vdwEJqvpbWQv25qAH1xn+f7em8/z8LWw5cJyuTWvz6JB29C40m8cYY6pShb6MVdX9qrrK/fw4sBloXMIhI4CZZ1NodSEiDGxbj6//fD4TrulCWkY2w6f8whNfbrAvbI0xXqdcY/QiEgcsAjqpakYR7TWBVKCVqh52b0sBjgAKvKWqU4p57zHAGIBmzZr12L272Nsfep3s3Hxe/CaZtxen0LVJFJNHdqdpdE2nyzLG+BGPTK8UkQhgNnBfUSHvNhRYcjrk3fqqandgCDBWRPoXdaCqTlHVBFVNiI0t8kbmXissOJDHL+vAW6N6sPO3E1w+6Wc+Wr6HvPwCp0szxpiyBb2IBOMK+RmqOqeEXYdzxrCNqu5z/0wDPgd6nl2p3m9Qxwb8Z1w/2tSP4LHP1zPolUV8u+kg3jizyRjjP0oNenFNJ3kX2KyqL5WwXxRwAfBloW3h7i9wEZFw4BJgQ0WL9mbN64bzyZ19eGtUDxS4Y3oid89YRUZ2rtOlGWP8VFnWuukLjALWi8ga97bHgGYAqvqme9uVwDeqeqLQsfWBz91TD4OAj1R1gScK92YiwqCODbioXT3e+TmFCQuT2TzpZ16/oQcdGtVyujxjjJ+xC6aqwIqUw4ybuYqjWbk8f3UXrogvadKSMcaUn61147CeLaL5+s/n061pbe77eA0Tv99m4/bGmCpjQV9FYiJCmT66J1fGN+alb7fy8Ox15NqsHGNMFfCr9eidFhoUyEvXdaVJnRpM+mE7+49l8/oN3W2BNGNMpbIz+iomIvz1krY8f3Vnlu44xLVvLmP/sZNOl2WM8WEW9A65/txmvHfLuaQeOcmVry1l077irkEzxpiKsaB3UP82sXxyZx8ArntrGYu2pjtckTHGF1nQO6xDo1p8PvY8mtSpwa3TVvLJyr1Ol2SM8TEW9F6gYVQNPr2rD+edU5eHZq9j0vfbnC7JGONDLOi9RGRYMFNvOZer4hvz4rdbeW9JitMlGWN8hE2v9CLBgQG8cE0XMk/l8dRXm4gOD2FYN7uK1hhTMXZG72WCAgOYOCKeXi2i+esna/lxS5rTJRljqjkLei8UFhzI2zcn0LZBJLdPT+Stn3bYkgnGmLNmQe+laoUFM2tMbwZ1rM+/5m/hjulJHMuypY6NMeVnQe/FIsOCeW1kd8YP7cB/k9O46o0lFvbGmHKzoPdyIsKtfVswfXRP9hzO4u6PkmwxNGNMuVjQVxPnnRPDs1d2Zsn2Qzz91SanyzHGVCM2vbIauTahKdvTMnlr0U5a14/gpj5xTpdkjKkGLOirmYcGt2NHeiZPzt1IvcgwBndq4HRJxhgvZ0M31UxggDBxRDxdmtTmz7NWs2zHIadLMsZ4uVKDXkSaisiPIrJZRDaKyL1F7DNARI6JyBr344lCbYNFJFlEtovII57ugD+qGRLEe7ecS7PomtwxPZENvx5zuiRjjBcryxl9HvBXVW0P9AbGikiHIvZbrKrd3I+nAUQkEHgNGAJ0AEYUc6wppzrhIXwwuidRNYK5eeoK1u496nRJxhgvVWrQq+p+VV3lfn4c2AyUdQGWnsB2Vd2pqjnALGDY2RZr/lfDqBp8eHsvaoYGcv2UZXy76aDTJRljvFC5xuhFJA6IB5YX0dxHRNaKyHwR6eje1hgovMB6KsX8khCRMSKSKCKJ6el2A46yahETzpw/9aVt/Uju/CCR95fucrokY4yXKXPQi0gEMBu4T1XPvO/dKqC5qnYFJgFfnD6siLcqctEWVZ2iqgmqmhAbG1vWsgwQGxnKzDG9ubBdfcbP3chXa/c5XZIxxouUKehFJBhXyM9Q1Tlntqtqhqpmup/PA4JFJAbXGXzTQrs2ASyFKkHNkCDeuLE7PZrX4ZHZ69ielul0ScYYL1GWWTcCvAtsVtWXitmngXs/RKSn+30PASuB1iLSQkRCgOHAXE8Vb/5XcGAAk0fGExocyN0zksjKyXO6JGOMFyjLGX1fYBRwYaHpk5eKyF0icpd7n2uADSKyFpgIDFeXPOAeYCGuL3E/UdWNldAP49YwqgavDu/GtrRM/vb5Blve2BhT+pWxqvozRY+1F95nMjC5mLZ5wLyzqs6clfNbx3LvRa155bttJMRFM7JXM6dLMsY4yK6M9VHjLmzN+a1jePKrjXZBlTF+zoLeRwUGCK9c34264SH8aUYSx07aOvbG+CsLeh9WNyKUySO7s/9oNg98utbG643xUxb0Pq5H8zo8eml7vt10kOcXJDtdjjHGAbZMsR+4rW8cO9MzefOnHdQND+GO/i2dLskYU4Us6P2AiPD0sE4cPZnLM/M2U7tmMNcmNC39QGOMT7Cg9xOBAcJL13XlWFYuj8xZT0xkKAPb1nO6LGNMFbAxej8SGhTIW6N60K5BJOM+Ws3Wg8edLskYUwUs6P1MeGgQb9+UQFhwIKPfX8nhEzlOl2SMqWQW9H6oUe0avH1TDw5mnOKuD5PIyStwuiRjTCWyoPdT8c3qMOGaLqxIOcw/v97kdDnGmEpkQe/HhnVrzB3nt2D6st3MtTXsjfFZFvR+7qHB7Uj4fQ17+3LWGF9kQe/nXGvYd6dGcCB/+nCVrWFvjA+yoDc0iApj4oh4tqdn8tic9bYmjjE+xoLeANC3VQz3X9yGL9bs46MVe5wuxxjjQRb05nf3DGxF/zaxPDV3E+tTbQ17Y3yFBb35XcDpNewjQrj7oySOZdka9sb4Agt68z+iw0N47QbXGvZ/nrWa/AIbrzemuis16EWkqYj8KCKbRWSjiNxbxD43iMg692OpiHQt1LZLRNa7byqe6OkOGM/r3qwOTw/rxE9b03l23manyzHGVFBZVq/MA/6qqqtEJBJIEpFvVbXw5ZQpwAWqekREhgBTgF6F2geq6m+eK9tUtpG9mrEt7Tjv/pxCq3oRjOhpNxg3proq9YxeVfer6ir38+PAZqDxGfssVdUj7pe/AE08Xaipeo9f2p7+bWL5+xcbWLbjkNPlGGPOUrnG6EUkDogHlpew22hgfqHXCnwjIkkiMqaE9x4jIokikpienl6eskwlCQoMYPLIeJrVrcm9s1ZzxFa6NKZaKnPQi0gEMBu4T1UzitlnIK6gf7jQ5r6q2h0YAowVkf5FHauqU1Q1QVUTYmNjy9wBU7lqhQUzaUQ8R7JyeNQupjKmWipT0ItIMK6Qn6Gqc4rZpwvwDjBMVX//O19V97l/pgGfAz0rWrSpWh0bRfHAJW1ZsPEAnyalOl2OMaacyjLrRoB3gc2q+lIx+zQD5gCjVHVroe3h7i9wEZFw4BJggycKN1Xr9vNb0rtlNE/N3cjuQyecLscYUw5lOaPvC4wCLnRPkVwjIpeKyF0icpd7nyeAusDrZ0yjrA/8LCJrgRXA16q6wNOdMJXPdc/ZbgQECPd9vIa8fLtZiTHVhXjjmGtCQoImJtqUe2/01dp9jJu5mnsvas39f2jjdDnGGDcRSVLVhKLa7MpYUy5DuzbiyvjGTP5xO0m7j5R+gDHGcRb0ptyeGtaRBrXCuP/jNWSesvXrjfF2FvSm3GqFBfPK8G6kHsniiS822JRLY7ycBb05K+fGRXPvRW2Ys/pX3lmc4nQ5xpgSlGWtG2OKNO7CViQfzODZ+ZtpGRvORe3rO12SMaYIdkZvzlpAgPDitd3o1CiKP89cTfIBu7m4Md7Igt5USI2QQN6+KYHw0CDGfJDIyZx8p0syxpzBgt5U2Ombi+8+lMWL3yQ7XY4x5gwW9MYjeresy429m/HukhRW7bH59cZ4Ewt64zGPDGlPo6gaPPTZOrJzbQjHGG9hQW88JiI0iGev6sz2tEwm/bDN6XKMMW4W9MajLmgTy7U9mvDmTzttCMcYL2FBbzzuiaEdaBgVxn2zbIkEY7yBBb3xuMiwYF6+3rVEwlNzNzpdjjF+z4LeVIpz46K5e0ArPk1KZf76/U6XY4xfs6A3lebei1vTtUkUj8xZT+qRLKfLMcZvWdCbShMcGMCrw+MpKFDGfrSanDy7K5UxTrCgN5UqLiacCdd2Ye3eozw7b7PT5RjjlyzoTaUb3Kkht/drwbSlu/hq7T6nyzHG75Qa9CLSVER+FJHNIrJRRO4tYh8RkYkisl1E1olI90Jtg0Uk2d32iKc7YKqHh4e0o0fzOjwyex3b0zKdLscYv1KWM/o84K+q2h7oDYwVkQ5n7DMEaO1+jAHeABCRQOA1d3sHYEQRxxo/EBwYwOSR8YQGB3L3jCSycmx+vTFVpdSgV9X9qrrK/fw4sBlofMZuw4Dp6vILUFtEGgI9ge2qulNVc4BZ7n2NH2oYVYNXh3djW1omf/vcbkFoTFUp1xi9iMQB8cDyM5oaA3sLvU51bytue1HvPUZEEkUkMT09vTxlmWrk/Nax3Oe+BeGslXtLP8AYU2FlDnoRiQBmA/epasaZzUUcoiVs//8bVaeoaoKqJsTGxpa1LFMNjbuwFf3bxDJ+7ka2HrS7UhlT2coU9CISjCvkZ6jqnCJ2SQWaFnrdBNhXwnbjxwIChJeu60pEaBAPfraO/AIbwjGmMpVl1o0A7wKbVfWlYnabC9zknn3TGzimqvuBlUBrEWkhIiHAcPe+xs/FRITy5B87snbvUab+nOJ0Ocb4tKAy7NMXGAWsF5E17m2PAc0AVPVNYB5wKbAdyAJudbflicg9wEIgEJiqqrbKlQFgaJeGfLV2H//+JpmLO9SnRUy40yUZ45PEG2c+JCQkaGJiotNlmCpwMCObi1/6ifYNazHrjt4EBBT1tY4xpjQikqSqCUW12ZWxxlH1a4Xx98s7sCLlMFOX2BCOMZXBgt447toeTfhDh/q8sCCZLQfOnNBljKkoC3rjOBHhuas6U6tGMPfOXGM3FjfGwyzojVeoGxHKhGu7kHzwOC8sSHa6HGN8igW98RoD29bj5j7NmbokhcXb7OpoYzzFgt54lUcvbU+rehE88OlajpzIcbocY3yCBb3xKmHBgbw6vBuHT+Tw6Jz1tvCZMR5gQW+8TsdGUTxwSVsWbDzAp0mpTpdjTLVnQW+80h3nt6RPy7o8NXcjuw+dcLocY6o1C3rjlQIChBev60pggHDfx2vIy7cbixtztizojddqVLsGz1zZmdV7jjLph+1Ol2NMtWVBb7za0K6NuCq+MZN/3E7S7iNOl2NMtWRBb7zek8M60qBWGPd/vIbMU3avWWPKy4LeeL1aYcG8MrwbqUeyePDTtRTYjUqMKRcLelMtnBsXzWOXtmf+hgO8/N1Wp8sxplopy41HjPEKo/u1YOvB40z6YTut6kUwrFuR95k3xpzBzuhNtSEi/POKzvRsEc2Dn61jzd6jTpdkTLVgQW+qlZCgAN68sQexEaH85WNb0tiYsrCgN9VOdHgIz13dmZ2/neCV77Y5XY4xXq/UoBeRqSKSJiIbiml/UETWuB8bRCRfRKLdbbtEZL27zW4Cazzm/NaxXJ/QlCmLdrDWhnCMKVFZzuinAYOLa1TVCaraTVW7AY8CP6nq4UK7DHS3F3nTWmPO1mOXtSc2MpSHPltHTp4tkWBMcUoNelVdBBwubT+3EcDMClVkTBlF1Qjm2Ss7k3zwOK9+b1MujSmOx8boRaQmrjP/2YU2K/CNiCSJyJhSjh8jIokikpiebncXMmVzUfv6XNujCa//dwf/TU5zuhxjvJInv4wdCiw5Y9imr6p2B4YAY0Wkf3EHq+oUVU1Q1YTY2FgPlmV83dPDOtG2fiT3f7yGX4+edLocY7yOJ4N+OGcM26jqPvfPNOBzoKcHP88YAGqEBPL6Dd3JzVfGzlhl4/XGnMEjQS8iUcAFwJeFtoWLSOTp58AlQJEzd4ypqJaxEbxwTRfW7D3Kc/O3OF2OMV6l1CUQRGQmMACIEZFUYDwQDKCqb7p3uxL4RlUL3wqoPvC5iJz+nI9UdYHnSjfmf13auSE392nO1CUpDGgbS/82NgRoDIB4482XExISNDHRpt2b8svOzWfopJ85djKXBff1Jzo8xOmSjKkSIpJU3DR2uzLW+JSw4EBeGd6NI1k5PDpnHd54ImNMVbOgNz6nY6MoHhzUloUbD/Lxyr1Ol2OM4yzojU+6vV9L+raqyxNzN7J6j92C0Pg3C3rjkwIChEkjutOgVhh3TE9in82vN37Mgt74rOjwEN69OYFTufnc/n4iWTl2v1njvX7ZeYjpy3ZVyq0yLeiNT2tdP5KJI+PZciCD+z9eY/ebNV4pKyePhz5bx7s/p3CqEi74s6A3Pm9g23o8flkHFm48yEvf2uJnxvu8+M1W9hzO4vmru1AjJNDj72/3jDV+4ba+cWw7eJzJP7ruN3tFvN1v1niHpN1HmLokhVG9m9O7Zd1K+Qw7ozd+QUR4elgnerWI5qHZ60jabTNxjPOyc/N5ePY6GkXV4OEh7Srtcyzojd84fb/ZhlFh3PlBEmkZ2U6XZPzcS99uZXtaJs9e1ZmI0MobYLGgN36lTngIU0YlkHkql3EzV5OXbytdGmd8vjqVKYt2ckOvZlxQyesyWdAbv9O2QSTPXtmZ5SmHedG+nDUOSNp9hIdnr6d3y2jGD+1Y6Z9nQW/80lXdmzCiZ1Pe+O8Ovt980OlyjB9JPZLFnR8k0jAqjDdu6EFIUOXHsAW98Vvjh3akY6Na/OWTtTZeb6pEdm4+d36QxKm8At69+VzqVNHqqhb0xm+FBQcycUQ8J3Pz+fuXG2ylS1PpnvpqExv3ZfDK9d1oVS+iyj7Xgt74tXNiI7j/4jYs3HiQeesPOF2O8WGzk1KZuWIPdw84h4va16/Sz7agN37vjvNb0LlxFE98uYHDJ3KcLsf4oC0HMnj8i/X0ahHNX/7Qpso/34Le+L2gwAAmXNuFjOxcnpy70YZwjEelHz/FnR8kERkWzKSR8QQFVn3sWtAbA7RrUItxF7Zm7tp9PLdgi4W98YiM7FxunrqCtIxTvDWqB/Uiwxypo9SgF5GpIpImIhuKaR8gIsdEZI378UShtsEikiwi20XkEU8Wboyn3TOwFaN6N+etn3byz683W9ibCsnOzeeO9xPZevA4b9zYne7N6jhWS1muuZ0GTAaml7DPYlW9vPAGEQkEXgP+AKQCK0VkrqpuOstajalUAQHC08M6EhQovPtzCvkFyvihHRARp0sz1UxBgXLfrDWs2HWYV67vxoC29Rytp9SgV9VFIhJ3Fu/dE9iuqjsBRGQWMAywoDdeS0R44vIOBAUIby9OITYylLEDWzldlqlm3lq0kwUbD/C3y9ozrJvzK6V6aoy+j4isFZH5InL6et7GQOE7M6e6txVJRMaISKKIJKanp3uoLGPKT0R47NL2/LFrIyYsTGbBBpt2acpu2Y5DTFi4hcu7NGR0vxZOlwN4JuhXAc1VtSswCfjCvb2ov3eLHfRU1SmqmqCqCbGxlbvAjzGlERFeuKYLXZvW5v6P17Dh12NOl2SqgbTj2YybuZq4mHCeu7qL1wz7VTjoVTVDVTPdz+cBwSISg+sMvmmhXZsA+yr6ecZUlbDgQN4e1YPaNYO5Y3oiacdtmQRTvFN5+Yz7aDWZp3J544YelbrscHlVOOhFpIG4f22JSE/3ex4CVgKtRaSFiIQAw4G5Ff08Y6pSvVphvH1TAkezchkzPYns3HynSzJeKL9A+cvHa1mecpjnr+5C2waRTpf0P8oyvXImsAxoKyKpIjJaRO4Skbvcu1wDbBCRtcBEYLi65AH3AAuBzcAnqrqxcrphTOXp1DiKl6/vypq9R3l49jqbdmn+h6ryxJcb+Hr9fh6/1Du+fD1TWWbdjCilfTKu6ZdFtc0D5p1dacZ4j8GdGvLgoLZMWJhMm/qRNhPH/O7lb7cyY/ke7rrgHO7o39LpcorkPYNIxni5uwecw7aDx5mwMJmI0CBuPi/O6ZKMwz5N3MvEH7ZzXUITHh7c1ulyimVBb0wZiQjPXd2FEzn5jJ+7kdQjWTw6pD0BAd4xs8JUrZW7DvPY5+vp1yqGZ67s7DUzbIpia90YUw5hwYG8eWMPbjkvjrcXpzD2o1X2Ba0f2ns4izs/SKJpnZq8NrI7wQ4sVFYe3l2dMV4oMEB48o8d+fvlHZi/4QDPztvsdEmmCv2WeYrR768kL7+Ad25OIKpmsNMllcqGbow5S6P7tWD/0ZO883MKA9vWY2A7Z9czMZVvR3omt763koMZ2Uy95VxaxlbdXaIqws7ojamABwa1pV2DSB78bC2/ZZ5yuhxTiVbuOszVbyzlxKk8Zo3pTd9WMU6XVGYW9MZUQFhwIK8OjycjO49HbI69z/p522/c8M5yomuGMOfu84h3cMnhs2FBb0wFtW0QySOD2/Hd5jSe+moTOXkFTpdkPGjt3qOM+SCRljHhzP7TeTSvG+50SeVmY/TGeMAt58Wx90gW7y3ZxbrUo0we2Z1GtWs4XZapoO1pmdzy3gqiw0N4/7ae1AkPcbqks2Jn9MZ4QECAMH5oR14b2Z3kA8e5bOJiVu467HRZpgJSj2Rx07vLCQwQPhzdi/q1nLkNoCdY0BvjQZd1achX4/pRp2YIt7230pY3rqb2HMri+rd+IfNUHtNu7UlcTPUbrinMgt4YD2sZG8GHt/ciMiyIW95bwa7fTjhdkimHXb+d4Popy8g8lcdHd/SmU+Mop0uqMAt6YypBo9o1mD66F/kFyqipy0nLsLXsq4Pdh1whn52bz0wfCXmwoDem0rSqF8G0W3tyKDOHm6au4NjJXKdLMiXIyM5l9PuJnMorYOaY3nRoVMvpkjzGgt6YStS1aW2mjEpgR3omt7+/kpM5ti6ON8rLL+Cej1az67cTvHFDD9o18J2QBwt6Yypdv9YxvHJ9PIm7j3DPR6vIzbd59t7mmXmbWbQ1nX9c0Yk+59R1uhyPs6A3pgpc1qUhTw/rxPdb0njg07XkWdh7hfwCZcLCLby3ZBe39W3BiJ7NnC6pUtgFU8ZUkVG9m3M8O5cXFiSTX6C8fH03r1/e1pelHz/FvbNWs3THIa5PaMpjl7ZzuqRKY0FvTBW6e0ArggKEZ+dtIb9AeXV4PCFBFvZVLWn3Ef70YRIZ2blMuKYL1yY0dbqkSlWWm4NPFZE0EdlQTPsNIrLO/VgqIl0Lte0SkfUiskZEEj1ZuDHV1Zj+5/CEey370ZbqR7kAAArdSURBVO+v5PCJHKdL8itfr9vPiLd/oWZIIF+M7evzIQ9lG6OfBgwuoT0FuEBVuwD/AKac0T5QVbupasLZlWiM77mtXwteuLoLy3ce5vKJi1m154jTJfk8VWXKoh2M/WgVXRpHMefuvj43u6Y4pQa9qi4Cil20Q1WXqurp/0p/AZp4qDZjfNp15zZl9p/OIzBQuO7NZUz9OcWWOa4kefkFjJ+7kWfnbeGyzg358PZeRFfTBcrOhqcHB0cD8wu9VuAbEUkSkTElHSgiY0QkUUQS09PTPVyWMd6pc5Mo/nPP+QxsV4+n/7OJu2esIiPbLqzypKycPO78IInpy3Yzpn9LJo2IJyw40OmyqpSU5QxCROKA/6hqpxL2GQi8DvRT1UPubY1UdZ+I1AO+Bca5/0IoUUJCgiYm2pC+8R+uYYWdvLAwmaZ1avD6DT186spMp6RlZHP79EQ2/HqMp/7YkVF94pwuqdKISFJxQ+QeOaMXkS7AO8Cw0yEPoKr73D/TgM+Bnp74PGN8jYhw5wXnMPOO3mTl5HPl60v4ZOVep8uqtk7m5DP5h21c+OJPbDuYyZRRCT4d8qWpcNCLSDNgDjBKVbcW2h4uIpGnnwOXAEXO3DHGuPRsEc28e88nIa4OD81exwOfrrVlE8rp63X7GfDvH/n3N1vpc05dvhrXj4s71He6LEeVOo9eRGYCA4AYEUkFxgPBAKr6JvAEUBd4XUQA8tx/PtQHPndvCwI+UtUFldAHY3xKTEQo02/rxavfbWXSj9tZl3qUCdd0pWvT2k6X5tVUlck/bOfFb7fSpUkUk0d259y4aKfL8gplGqOvajZGb4zLoq3pPPTZOtKOZ3PH+S25/w9t/O6LxLLIzS/gb59v4OPEvVzRrRHPX9OF0CD/+udU6WP0xpjK0b9NLN/8pT/Xn9uUtxbtZMiri1mRYrcoLGzPoSxufGc5HyfuZdyFrXj5+m5+F/KlsaA3xsvVCgvmX1d1YcbtvcgrKOC6t5Yx/ssNnDiV53RpjsovUKb+nMKgVxaxcV8G/762K3+9pC3u4WJTiK11Y0w10bdVDAvv68+EhclMW7qL7zan8dDgtgzt0oiAAP8Jt9z8Auat38/bi3ey4dcMBraN5ZkrO9Oodg2nS/NaNkZvTDWUuOsw4+duZOO+DDo1rsWjQ9rTt1WM02VVqpM5+UxbuotpS1M4mHGKljHhjLuoFVd0a2xn8ZQ8Rm9Bb0w1VVCgzF27jwkLk/n16Emu6NaIJ4Z29LlL+/PyC/gsKZWXv9vKwYxTnN86htv6teCC1rF+9ZdMaUoKehu6MaaaCggQrohvzOBODXjjvzt47cftLN72G+P/2JFBHetX+y8kVZUftqTx3PwtbEvLJL5ZbZsyeZbsjN4YH7F5fwYPfbaO9b8eIyQwgI6Na9G9WR1G9W5OXEy40+WVy+o9R/jX/C2sSDlMi5hwHhrUlsGdGtgQTQls6MYYP5GXX8D3W9JYtfsIq/ccZU3qUQoKlBt6NWPcRa2JiQh1usRi5Rco3246wNQlu1iRcpiYiBDuvbgNw89tanfiKgMLemP8VNrxbF79bhuzVu6lRnAgd/ZvyejzW1AzxPlRW1Vl0/4Mkty/lH7ZeYj9x7JpUqcGN/eJY0SvZkSEOl9ndWFBb4yf25GeyfPzt/DNpoPUiwzlvovbcEV8I8cCf33qMf41fzNLd7jWQIyNDCW+aW2u6t6EP3SoT6B9yVpuFvTGGMA1LfNf87eQtPsIgQFC2/qRxDerTcvYCBpGhdEgKox2DSI9+gtgfeoxvt9ykIICV9bsSD/B1+v3U6dmMGMHtmJwpwY0rl3Dxt8ryGbdGGMASIiL5rO7+rBk+yGWpxxi9Z6jzF2zj+OFrrINCw5gQJt6DOncgHPjoqkXGUpQOcbICwqUQydyWJFymPeWpJC423UDutM5XjM4kLEDz+HOC86hVliwR/tnimZBb4yfERH6tY6hX2vXBVaqytGsXPYfy2bf0ZMs3pbO/A0HWLDxAAAB4hpaiQwLprRz7pO5+aRlnCInvwCAptE1+PvlHbg2oYmFuoNs6MYY8/8UFCir9x4h+UAmBzKy2X/0JCdySl9bJzQokPq1wmgYFUaLmHD6toqx8fYqYkM3xphyCQgQejSPpkdzuzjJF9jkVGOM8XEW9MYY4+Ms6I0xxsdZ0BtjjI8rNehFZKqIpInIhmLaRUQmish2EVknIt0LtQ0WkWR32yOeLNwYY0zZlOWMfhowuIT2IUBr92MM8AaAiAQCr7nbOwAjRKRDRYo1xhhTfqUGvaouAkq6G/EwYLq6/ALUFpGGQE9gu6ruVNUcYJZ7X2OMMVXIE2P0jYG9hV6nurcVt71IIjJGRBJFJDE9Pd0DZRljjAHPXDBV1GVvWsL2IqnqFGAKgIiki8jus6wnBvjtLI+trvyxz+Cf/fbHPoN/9ru8fW5eXIMngj4VaFrodRNgHxBSzPZSqWrs2RYjIonFXQbsq/yxz+Cf/fbHPoN/9tuTffbE0M1c4Cb37JvewDFV3Q+sBFqLSAsRCQGGu/c1xhhThUo9oxeRmcAAIEZEUoHxQDCAqr4JzAMuBbYDWcCt7rY8EbkHWAgEAlNVdWMl9MEYY0wJSg16VR1RSrsCY4tpm4frF0FVmlLFn+cN/LHP4J/99sc+g3/222N99splio0xxniOLYFgjDE+zoLeGGN8nM8Evb+sqyMiTUXkRxHZLCIbReRe9/ZoEflWRLa5f9ZxulZPE5FAEVktIv9xv/aHPtcWkc9EZIv733kfX++3iNzv/m97g4jMFJEwX+xzUeuIldRPEXnUnW/JIjKoPJ/lE0HvZ+vq5AF/VdX2QG9grLuvjwDfq2pr4Hv3a19zL7C50Gt/6POrwAJVbQd0xdV/n+23iDQG/gwkqGonXDP2huObfZ7G/19HrMh+uv8fHw50dB/zujv3ysQngh4/WldHVfer6ir38+O4/sdvjKu/77t3ex+4wpkKK4eINAEuA94ptNnX+1wL6A+8C6CqOap6FB/vN67ZgDVEJAioietCS5/rczHriBXXz2HALFU9paopuKaz9yzrZ/lK0JdrXR1fISJxQDywHKjvvlAN9896zlVWKV4BHgIKCm3z9T63BNKB99xDVu+ISDg+3G9V/RX4N7AH2I/rAsxv8OE+n6G4flYo43wl6Mu1ro4vEJEIYDZwn6pmOF1PZRKRy4E0VU1yupYqFgR0B95Q1XjgBL4xZFEs95j0MKAF0AgIF5Ebna3KK1Qo43wl6Itbb8cniUgwrpCfoapz3JsPupeHxv0zzan6KkFf4I8isgvXsNyFIvIhvt1ncP13naqqy92vP8MV/L7c74uBFFVNV9VcYA5wHr7d58KK62eFMs5Xgt5v1tUREcE1ZrtZVV8q1DQXuNn9/Gbgy6qurbKo6qOq2kRV43D9u/1BVW/Eh/sMoKoHgL0i0ta96SJgE77d7z1AbxGp6f5v/SJc30P5cp8LK66fc4HhIhIqIi1w3ehpRZnfVVV94oFrvZ2twA7gcafrqcR+9sP1J9s6YI37cSlQF9e39NvcP6OdrrWS+j8A+I/7uc/3GegGJLr/fX8B1PH1fgNPAVuADcAHQKgv9hmYiet7iFxcZ+yjS+on8Lg735KBIeX5LFsCwRhjfJyvDN0YY4wphgW9Mcb4OAt6Y4zxcRb0xhjj4yzojTHGx1nQG2OMj7OgN8YYH/d/FnnYQgey0AoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(5,10)\n",
    "        self.fc2 = nn.Linear(10,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "allloss = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs,Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    allloss.append(loss.item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(allloss)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8587456434762569 1.0\n",
      "0.8757430970606378 0.0\n",
      "0.8889425550952063 1.0\n",
      "0.8611343977512539 0.0\n",
      "0.8981988263532084 1.0\n",
      "0.8897444624532463 1.0\n",
      "0.8906821441102999 0.0\n",
      "0.8814461393025347 0.0\n",
      "0.8796550411609343 1.0\n",
      "0.8742112583482698 1.0\n",
      "0.8958446572252121 1.0\n",
      "0.8813613916116556 0.0\n",
      "0.8622744780573498 0.0\n",
      "0.875737370380965 0.0\n",
      "0.8660514898390942 0.0\n",
      "0.8908894452012738 1.0\n",
      "0.8637027255493858 0.0\n",
      "0.881097201914139 1.0\n",
      "0.8712460519020662 0.0\n",
      "0.8855693817950465 1.0\n",
      "0.8993405622788174 1.0\n",
      "0.9038439841886858 1.0\n",
      "0.9094512322961329 1.0\n",
      "0.890034192814933 1.0\n",
      "0.8988739944512354 1.0\n",
      "0.8675434685350698 0.0\n",
      "0.8876729876437612 1.0\n",
      "0.8855129534188321 1.0\n",
      "0.8925918007458836 1.0\n",
      "0.8680091735737142 0.0\n",
      "0.8730747695133059 0.0\n",
      "0.8749800296165517 0.0\n",
      "0.8665033266599836 0.0\n",
      "0.8839324786477778 0.0\n",
      "0.9008271029279078 1.0\n",
      "0.8709848197987933 0.0\n",
      "0.8755143590924563 0.0\n",
      "0.8897672925797194 1.0\n",
      "0.8744603576520538 0.0\n",
      "0.8751611899971453 0.0\n"
     ]
    }
   ],
   "source": [
    "#From scratch\n",
    "weights = [-0.1, 0.20, -0.23, -0.1, 0.20, -0.23, -0.1, 0.20, -0.23] #Weights are randomly chosen\n",
    "\n",
    "import math\n",
    "def sigmoid(z): #So the sigmoid doesnt go to infinity:\n",
    "    if(z<-100): \n",
    "        return 0\n",
    "    if(z>100):\n",
    "        return 1\n",
    "    return 1.0/math.exp(-z)\n",
    "\n",
    "#Makes the first hidden layer of the multilayer perceptron\n",
    "def firstLayer(row,weights):\n",
    "    activation_1 = weights[0]*1 #The bias = 1\n",
    "    activation_1 += weights[1]*row[0] #Given the input data, multiply the first number with weight\n",
    "    activation_1 += weights[2]*row[1]\n",
    "\n",
    "    activation_2 = weights[3]*1\n",
    "    activation_2 += weights[4]*row[2]\n",
    "    activation_2 += weights[5]*row[3]\n",
    "    return sigmoid(activation_1),sigmoid(activation_2) #After doing linear multiplication -> return the sigmoid of activation 1 and 2\n",
    "\n",
    "#Do the same thing for the second layer, the only difference is different weights \n",
    "def secondLayer(row,weights):\n",
    "    activation_3 = weights[6]\n",
    "    activation_3 += weights[7]*row[0]\n",
    "    activation_3 += weights[8]*row[1]\n",
    "    return sigmoid(activation_3)\n",
    "\n",
    "#Then we want to do the actual prediction\n",
    "def predict(row,weights): \n",
    "    input_layer = row\n",
    "    first_layer = firstLayer(input_layer,weights) #Whatever comes out of the first layer function using data from input layer \n",
    "    second_layer = secondLayer(first_layer,weights) #Whatever comes out of the second layer function using data from the first layer \n",
    "    return second_layer,first_layer #Return both\n",
    "\n",
    "#For every data in the dataset\n",
    "for d in training_dataset:\n",
    "    print(predict(d,weights)[0],d[-1])   #Prints y_hat and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Learning rate 0.0001 Error -15.201852800535265\n",
      "Epoch 1 Learning rate 0.0001 Error -15.065223046845453\n",
      "Epoch 2 Learning rate 0.0001 Error -14.93080011671281\n",
      "Epoch 3 Learning rate 0.0001 Error -14.79852803522042\n",
      "Epoch 4 Learning rate 0.0001 Error -14.668352756779203\n",
      "Epoch 5 Learning rate 0.0001 Error -14.54022208150549\n",
      "Epoch 6 Learning rate 0.0001 Error -14.414085575949793\n",
      "Epoch 7 Learning rate 0.0001 Error -14.289894497913183\n",
      "Epoch 8 Learning rate 0.0001 Error -14.167601725106044\n",
      "Epoch 9 Learning rate 0.0001 Error -14.047161687420598\n",
      "Epoch 10 Learning rate 0.0001 Error -13.928530302604043\n",
      "Epoch 11 Learning rate 0.0001 Error -13.811664915133655\n",
      "Epoch 12 Learning rate 0.0001 Error -13.696524238108132\n",
      "Epoch 13 Learning rate 0.0001 Error -13.583068297982019\n",
      "Epoch 14 Learning rate 0.0001 Error -13.471258381981151\n",
      "Epoch 15 Learning rate 0.0001 Error -13.361056988047622\n",
      "Epoch 16 Learning rate 0.0001 Error -13.252427777172644\n",
      "Epoch 17 Learning rate 0.0001 Error -13.145335527984583\n",
      "Epoch 18 Learning rate 0.0001 Error -13.039746093467855\n",
      "Epoch 19 Learning rate 0.0001 Error -12.935626359696245\n",
      "Epoch 20 Learning rate 0.0001 Error -12.832944206471431\n",
      "Epoch 21 Learning rate 0.0001 Error -12.731668469764147\n",
      "Epoch 22 Learning rate 0.0001 Error -12.631768905861962\n",
      "Epoch 23 Learning rate 0.0001 Error -12.533216157133099\n",
      "Epoch 24 Learning rate 0.0001 Error -12.43598171932157\n",
      "Epoch 25 Learning rate 0.0001 Error -12.340037910293786\n",
      "Epoch 26 Learning rate 0.0001 Error -12.245357840161468\n",
      "Epoch 27 Learning rate 0.0001 Error -12.151915382710289\n",
      "Epoch 28 Learning rate 0.0001 Error -12.059685148067775\n",
      "Epoch 29 Learning rate 0.0001 Error -11.968642456547656\n",
      "Epoch 30 Learning rate 0.0001 Error -11.878763313611815\n",
      "Epoch 31 Learning rate 0.0001 Error -11.79002438589403\n",
      "Epoch 32 Learning rate 0.0001 Error -11.702402978233037\n",
      "Epoch 33 Learning rate 0.0001 Error -11.615877011665436\n",
      "Epoch 34 Learning rate 0.0001 Error -11.53042500233157\n",
      "Epoch 35 Learning rate 0.0001 Error -11.44602604125026\n",
      "Epoch 36 Learning rate 0.0001 Error -11.362659774920697\n",
      "Epoch 37 Learning rate 0.0001 Error -11.280306386711938\n",
      "Epoch 38 Learning rate 0.0001 Error -11.198946579002778\n",
      "Epoch 39 Learning rate 0.0001 Error -11.118561556036733\n",
      "Epoch 40 Learning rate 0.0001 Error -11.039133007458643\n",
      "Epoch 41 Learning rate 0.0001 Error -10.960643092501376\n",
      "Epoch 42 Learning rate 0.0001 Error -10.883074424792625\n",
      "Epoch 43 Learning rate 0.0001 Error -10.806410057753444\n",
      "Epoch 44 Learning rate 0.0001 Error -10.730633470561683\n",
      "Epoch 45 Learning rate 0.0001 Error -10.655728554654686\n",
      "Epoch 46 Learning rate 0.0001 Error -10.581679600747337\n",
      "Epoch 47 Learning rate 0.0001 Error -10.50847128634209\n",
      "Epoch 48 Learning rate 0.0001 Error -10.436088663709697\n",
      "Epoch 49 Learning rate 0.0001 Error -10.364517148319552\n",
      "Epoch 50 Learning rate 0.0001 Error -10.293742507700205\n",
      "Epoch 51 Learning rate 0.0001 Error -10.22375085071135\n",
      "Epoch 52 Learning rate 0.0001 Error -10.154528617209536\n",
      "Epoch 53 Learning rate 0.0001 Error -10.086062568090725\n",
      "Epoch 54 Learning rate 0.0001 Error -10.018339775693677\n",
      "Epoch 55 Learning rate 0.0001 Error -9.951347614548865\n",
      "Epoch 56 Learning rate 0.0001 Error -9.885073752458469\n",
      "Epoch 57 Learning rate 0.0001 Error -9.819506141893495\n",
      "Epoch 58 Learning rate 0.0001 Error -9.754633011695022\n",
      "Epoch 59 Learning rate 0.0001 Error -9.690442859066826\n",
      "Epoch 60 Learning rate 0.0001 Error -9.626924441847615\n",
      "Epoch 61 Learning rate 0.0001 Error -9.564066771051362\n",
      "Epoch 62 Learning rate 0.0001 Error -9.501859103664911\n",
      "Epoch 63 Learning rate 0.0001 Error -9.440290935692543\n",
      "Epoch 64 Learning rate 0.0001 Error -9.379351995437483\n",
      "Epoch 65 Learning rate 0.0001 Error -9.319032237011081\n",
      "Epoch 66 Learning rate 0.0001 Error -9.259321834060508\n",
      "Epoch 67 Learning rate 0.0001 Error -9.20021117370653\n",
      "Epoch 68 Learning rate 0.0001 Error -9.14169085068297\n",
      "Epoch 69 Learning rate 0.0001 Error -9.083751661670167\n",
      "Epoch 70 Learning rate 0.0001 Error -9.02638459981483\n",
      "Epoch 71 Learning rate 0.0001 Error -8.96958084942924\n",
      "Epoch 72 Learning rate 0.0001 Error -8.913331780862755\n",
      "Epoch 73 Learning rate 0.0001 Error -8.857628945539274\n",
      "Epoch 74 Learning rate 0.0001 Error -8.8024640711543\n",
      "Epoch 75 Learning rate 0.0001 Error -8.747829057025573\n",
      "Epoch 76 Learning rate 0.0001 Error -8.693715969591564\n",
      "Epoch 77 Learning rate 0.0001 Error -8.640117038052399\n",
      "Epoch 78 Learning rate 0.0001 Error -8.58702465014781\n",
      "Epoch 79 Learning rate 0.0001 Error -8.534431348067232\n",
      "Epoch 80 Learning rate 0.0001 Error -8.482329824487087\n",
      "Epoch 81 Learning rate 0.0001 Error -8.430712918730721\n",
      "Epoch 82 Learning rate 0.0001 Error -8.379573613046503\n",
      "Epoch 83 Learning rate 0.0001 Error -8.328905028999879\n",
      "Epoch 84 Learning rate 0.0001 Error -8.278700423975238\n",
      "Epoch 85 Learning rate 0.0001 Error -8.22895318778378\n",
      "Epoch 86 Learning rate 0.0001 Error -8.179656839373537\n",
      "Epoch 87 Learning rate 0.0001 Error -8.130805023638029\n",
      "Epoch 88 Learning rate 0.0001 Error -8.082391508320017\n",
      "Epoch 89 Learning rate 0.0001 Error -8.03441018100714\n",
      "Epoch 90 Learning rate 0.0001 Error -7.986855046216139\n",
      "Epoch 91 Learning rate 0.0001 Error -7.939720222562722\n",
      "Epoch 92 Learning rate 0.0001 Error -7.892999940014052\n",
      "Epoch 93 Learning rate 0.0001 Error -7.846688537221048\n",
      "Epoch 94 Learning rate 0.0001 Error -7.800780458927845\n",
      "Epoch 95 Learning rate 0.0001 Error -7.755270253455727\n",
      "Epoch 96 Learning rate 0.0001 Error -7.710152570259096\n",
      "Epoch 97 Learning rate 0.0001 Error -7.6654221575509895\n",
      "Epoch 98 Learning rate 0.0001 Error -7.621073859995951\n",
      "Epoch 99 Learning rate 0.0001 Error -7.5771026164678865\n",
      "Epoch 100 Learning rate 0.0001 Error -7.533503457870833\n",
      "Epoch 101 Learning rate 0.0001 Error -7.490271505020621\n",
      "Epoch 102 Learning rate 0.0001 Error -7.447401966585319\n",
      "Epoch 103 Learning rate 0.0001 Error -7.404890137082655\n",
      "Epoch 104 Learning rate 0.0001 Error -7.362731394932593\n",
      "Epoch 105 Learning rate 0.0001 Error -7.320921200563114\n",
      "Epoch 106 Learning rate 0.0001 Error -7.279455094567739\n",
      "Epoch 107 Learning rate 0.0001 Error -7.238328695912966\n",
      "Epoch 108 Learning rate 0.0001 Error -7.197537700194097\n",
      "Epoch 109 Learning rate 0.0001 Error -7.157077877937973\n",
      "Epoch 110 Learning rate 0.0001 Error -7.116945072951024\n",
      "Epoch 111 Learning rate 0.0001 Error -7.077135200711358\n",
      "Epoch 112 Learning rate 0.0001 Error -7.0376442468034215\n",
      "Epoch 113 Learning rate 0.0001 Error -6.998468265393936\n",
      "Epoch 114 Learning rate 0.0001 Error -6.959603377747853\n",
      "Epoch 115 Learning rate 0.0001 Error -6.921045770783075\n",
      "Epoch 116 Learning rate 0.0001 Error -6.882791695662786\n",
      "Epoch 117 Learning rate 0.0001 Error -6.844837466424217\n",
      "Epoch 118 Learning rate 0.0001 Error -6.807179458642769\n",
      "Epoch 119 Learning rate 0.0001 Error -6.769814108130404\n",
      "Epoch 120 Learning rate 0.0001 Error -6.732737909667298\n",
      "Epoch 121 Learning rate 0.0001 Error -6.695947415765734\n",
      "Epoch 122 Learning rate 0.0001 Error -6.659439235465307\n",
      "Epoch 123 Learning rate 0.0001 Error -6.623210033158491\n",
      "Epoch 124 Learning rate 0.0001 Error -6.58725652744568\n",
      "Epoch 125 Learning rate 0.0001 Error -6.551575490018841\n",
      "Epoch 126 Learning rate 0.0001 Error -6.516163744572941\n",
      "Epoch 127 Learning rate 0.0001 Error -6.48101816574432\n",
      "Epoch 128 Learning rate 0.0001 Error -6.446135678075272\n",
      "Epoch 129 Learning rate 0.0001 Error -6.411513255004025\n",
      "Epoch 130 Learning rate 0.0001 Error -6.377147917879447\n",
      "Epoch 131 Learning rate 0.0001 Error -6.343036734999707\n",
      "Epoch 132 Learning rate 0.0001 Error -6.309176820674275\n",
      "Epoch 133 Learning rate 0.0001 Error -6.275565334308538\n",
      "Epoch 134 Learning rate 0.0001 Error -6.242199479510431\n",
      "Epoch 135 Learning rate 0.0001 Error -6.209076503218467\n",
      "Epoch 136 Learning rate 0.0001 Error -6.1761936948505305\n",
      "Epoch 137 Learning rate 0.0001 Error -6.1435483854728945\n",
      "Epoch 138 Learning rate 0.0001 Error -6.111137946988857\n",
      "Epoch 139 Learning rate 0.0001 Error -6.07895979134652\n",
      "Epoch 140 Learning rate 0.0001 Error -6.0470113697651104\n",
      "Epoch 141 Learning rate 0.0001 Error -6.01529017197937\n",
      "Epoch 142 Learning rate 0.0001 Error -5.983793725501533\n",
      "Epoch 143 Learning rate 0.0001 Error -5.952519594900374\n",
      "Epoch 144 Learning rate 0.0001 Error -5.921465381096904\n",
      "Epoch 145 Learning rate 0.0001 Error -5.89062872067624\n",
      "Epoch 146 Learning rate 0.0001 Error -5.860007285215233\n",
      "Epoch 147 Learning rate 0.0001 Error -5.8295987806253695\n",
      "Epoch 148 Learning rate 0.0001 Error -5.799400946510664\n",
      "Epoch 149 Learning rate 0.0001 Error -5.76941155554001\n",
      "Epoch 150 Learning rate 0.0001 Error -5.739628412833682\n",
      "Epoch 151 Learning rate 0.0001 Error -5.710049355363621\n",
      "Epoch 152 Learning rate 0.0001 Error -5.680672251367085\n",
      "Epoch 153 Learning rate 0.0001 Error -5.651494999773371\n",
      "Epoch 154 Learning rate 0.0001 Error -5.622515529643242\n",
      "Epoch 155 Learning rate 0.0001 Error -5.593731799620724\n",
      "Epoch 156 Learning rate 0.0001 Error -5.565141797396974\n",
      "Epoch 157 Learning rate 0.0001 Error -5.536743539185903\n",
      "Epoch 158 Learning rate 0.0001 Error -5.508535069211222\n",
      "Epoch 159 Learning rate 0.0001 Error -5.480514459204656\n",
      "Epoch 160 Learning rate 0.0001 Error -5.452679807915044\n",
      "Epoch 161 Learning rate 0.0001 Error -5.425029240628002\n",
      "Epoch 162 Learning rate 0.0001 Error -5.397560908695937\n",
      "Epoch 163 Learning rate 0.0001 Error -5.370272989078118\n",
      "Epoch 164 Learning rate 0.0001 Error -5.343163683890579\n",
      "Epoch 165 Learning rate 0.0001 Error -5.316231219965545\n",
      "Epoch 166 Learning rate 0.0001 Error -5.289473848420247\n",
      "Epoch 167 Learning rate 0.0001 Error -5.262889844234793\n",
      "Epoch 168 Learning rate 0.0001 Error -5.236477505838943\n",
      "Epoch 169 Learning rate 0.0001 Error -5.210235154707515\n",
      "Epoch 170 Learning rate 0.0001 Error -5.184161134964262\n",
      "Epoch 171 Learning rate 0.0001 Error -5.158253812993949\n",
      "Epoch 172 Learning rate 0.0001 Error -5.132511577062512\n",
      "Epoch 173 Learning rate 0.0001 Error -5.106932836944994\n",
      "Epoch 174 Learning rate 0.0001 Error -5.081516023561232\n",
      "Epoch 175 Learning rate 0.0001 Error -5.056259588618909\n",
      "Epoch 176 Learning rate 0.0001 Error -5.031162004263947\n",
      "Epoch 177 Learning rate 0.0001 Error -5.006221762738022\n",
      "Epoch 178 Learning rate 0.0001 Error -4.98143737604299\n",
      "Epoch 179 Learning rate 0.0001 Error -4.956807375612106\n",
      "Epoch 180 Learning rate 0.0001 Error -4.932330311987882\n",
      "Epoch 181 Learning rate 0.0001 Error -4.908004754506376\n",
      "Epoch 182 Learning rate 0.0001 Error -4.883829290987812\n",
      "Epoch 183 Learning rate 0.0001 Error -4.85980252743336\n",
      "Epoch 184 Learning rate 0.0001 Error -4.835923087727941\n",
      "Epoch 185 Learning rate 0.0001 Error -4.812189613348876\n",
      "Epoch 186 Learning rate 0.0001 Error -4.788600763080342\n",
      "Epoch 187 Learning rate 0.0001 Error -4.765155212733363\n",
      "Epoch 188 Learning rate 0.0001 Error -4.741851654871366\n",
      "Epoch 189 Learning rate 0.0001 Error -4.718688798541012\n",
      "Epoch 190 Learning rate 0.0001 Error -4.695665369008291\n",
      "Epoch 191 Learning rate 0.0001 Error -4.672780107499786\n",
      "Epoch 192 Learning rate 0.0001 Error -4.65003177094883\n",
      "Epoch 193 Learning rate 0.0001 Error -4.627419131746626\n",
      "Epoch 194 Learning rate 0.0001 Error -4.604940977498117\n",
      "Epoch 195 Learning rate 0.0001 Error -4.582596110782523\n",
      "Epoch 196 Learning rate 0.0001 Error -4.560383348918429\n",
      "Epoch 197 Learning rate 0.0001 Error -4.538301523733363\n",
      "Epoch 198 Learning rate 0.0001 Error -4.516349481337715\n",
      "Epoch 199 Learning rate 0.0001 Error -4.494526081902924\n",
      "Epoch 200 Learning rate 0.0001 Error -4.472830199443818\n",
      "Epoch 201 Learning rate 0.0001 Error -4.451260721605071\n",
      "Epoch 202 Learning rate 0.0001 Error -4.429816549451628\n",
      "Epoch 203 Learning rate 0.0001 Error -4.408496597263007\n",
      "Epoch 204 Learning rate 0.0001 Error -4.387299792331479\n",
      "Epoch 205 Learning rate 0.0001 Error -4.366225074763919\n",
      "Epoch 206 Learning rate 0.0001 Error -4.345271397287339\n",
      "Epoch 207 Learning rate 0.0001 Error -4.324437725057992\n",
      "Epoch 208 Learning rate 0.0001 Error -4.303723035473984\n",
      "Epoch 209 Learning rate 0.0001 Error -4.283126317991274\n",
      "Epoch 210 Learning rate 0.0001 Error -4.262646573943055\n",
      "Epoch 211 Learning rate 0.0001 Error -4.242282816362394\n",
      "Epoch 212 Learning rate 0.0001 Error -4.222034069808093\n",
      "Epoch 213 Learning rate 0.0001 Error -4.201899370193689\n",
      "Epoch 214 Learning rate 0.0001 Error -4.1818777646194984\n",
      "Epoch 215 Learning rate 0.0001 Error -4.161968311207702\n",
      "Epoch 216 Learning rate 0.0001 Error -4.142170078940371\n",
      "Epoch 217 Learning rate 0.0001 Error -4.122482147500358\n",
      "Epoch 218 Learning rate 0.0001 Error -4.102903607115024\n",
      "Epoch 219 Learning rate 0.0001 Error -4.083433558402744\n",
      "Epoch 220 Learning rate 0.0001 Error -4.064071112222074\n",
      "Epoch 221 Learning rate 0.0001 Error -4.044815389523621\n",
      "Epoch 222 Learning rate 0.0001 Error -4.025665521204497\n",
      "Epoch 223 Learning rate 0.0001 Error -4.006620647965281\n",
      "Epoch 224 Learning rate 0.0001 Error -3.9876799201695188\n",
      "Epoch 225 Learning rate 0.0001 Error -3.968842497705621\n",
      "Epoch 226 Learning rate 0.0001 Error -3.9501075498511975\n",
      "Epoch 227 Learning rate 0.0001 Error -3.931474255139686\n",
      "Epoch 228 Learning rate 0.0001 Error -3.91294180122933\n",
      "Epoch 229 Learning rate 0.0001 Error -3.894509384774334\n",
      "Epoch 230 Learning rate 0.0001 Error -3.8761762112982874\n",
      "Epoch 231 Learning rate 0.0001 Error -3.8579414950697144\n",
      "Epoch 232 Learning rate 0.0001 Error -3.8398044589797484\n",
      "Epoch 233 Learning rate 0.0001 Error -3.8217643344218697\n",
      "Epoch 234 Learning rate 0.0001 Error -3.803820361173697\n",
      "Epoch 235 Learning rate 0.0001 Error -3.7859717872807446\n",
      "Epoch 236 Learning rate 0.0001 Error -3.768217868942157\n",
      "Epoch 237 Learning rate 0.0001 Error -3.750557870398334\n",
      "Epoch 238 Learning rate 0.0001 Error -3.732991063820455\n",
      "Epoch 239 Learning rate 0.0001 Error -3.715516729201823\n",
      "Epoch 240 Learning rate 0.0001 Error -3.6981341542510364\n",
      "Epoch 241 Learning rate 0.0001 Error -3.6808426342868987\n",
      "Epoch 242 Learning rate 0.0001 Error -3.6636414721350836\n",
      "Epoch 243 Learning rate 0.0001 Error -3.6465299780265124\n",
      "Epoch 244 Learning rate 0.0001 Error -3.6295074694973626\n",
      "Epoch 245 Learning rate 0.0001 Error -3.6125732712907475\n",
      "Epoch 246 Learning rate 0.0001 Error -3.5957267152599743\n",
      "Epoch 247 Learning rate 0.0001 Error -3.578967140273403\n",
      "Epoch 248 Learning rate 0.0001 Error -3.562293892120818\n",
      "Epoch 249 Learning rate 0.0001 Error -3.5457063234213426\n",
      "Epoch 250 Learning rate 0.0001 Error -3.529203793532829\n",
      "Epoch 251 Learning rate 0.0001 Error -3.5127856684626915\n",
      "Epoch 252 Learning rate 0.0001 Error -3.496451320780186\n",
      "Epoch 253 Learning rate 0.0001 Error -3.4802001295300986\n",
      "Epoch 254 Learning rate 0.0001 Error -3.4640314801477894\n",
      "Epoch 255 Learning rate 0.0001 Error -3.447944764375604\n",
      "Epoch 256 Learning rate 0.0001 Error -3.431939380180603\n",
      "Epoch 257 Learning rate 0.0001 Error -3.416014731673605\n",
      "Epoch 258 Learning rate 0.0001 Error -3.400170229029485\n",
      "Epoch 259 Learning rate 0.0001 Error -3.384405288408744\n",
      "Epoch 260 Learning rate 0.0001 Error -3.3687193318803113\n",
      "Epoch 261 Learning rate 0.0001 Error -3.353111787345532\n",
      "Epoch 262 Learning rate 0.0001 Error -3.3375820884633565\n",
      "Epoch 263 Learning rate 0.0001 Error -3.3221296745767046\n",
      "Epoch 264 Learning rate 0.0001 Error -3.306753990639951\n",
      "Epoch 265 Learning rate 0.0001 Error -3.2914544871475493\n",
      "Epoch 266 Learning rate 0.0001 Error -3.2762306200637257\n",
      "Epoch 267 Learning rate 0.0001 Error -3.261081850753306\n",
      "Epoch 268 Learning rate 0.0001 Error -3.2460076459135614\n",
      "Epoch 269 Learning rate 0.0001 Error -3.2310074775071116\n",
      "Epoch 270 Learning rate 0.0001 Error -3.2160808226958575\n",
      "Epoch 271 Learning rate 0.0001 Error -3.201227163775915\n",
      "Epoch 272 Learning rate 0.0001 Error -3.18644598811352\n",
      "Epoch 273 Learning rate 0.0001 Error -3.171736788081954\n",
      "Epoch 274 Learning rate 0.0001 Error -3.157099060999349\n",
      "Epoch 275 Learning rate 0.0001 Error -3.142532309067497\n",
      "Epoch 276 Learning rate 0.0001 Error -3.128036039311529\n",
      "Epoch 277 Learning rate 0.0001 Error -3.1136097635205275\n",
      "Epoch 278 Learning rate 0.0001 Error -3.099252998189028\n",
      "Epoch 279 Learning rate 0.0001 Error -3.084965264459365\n",
      "Epoch 280 Learning rate 0.0001 Error -3.0707460880648836\n",
      "Epoch 281 Learning rate 0.0001 Error -3.0565949992740347\n",
      "Epoch 282 Learning rate 0.0001 Error -3.042511532835225\n",
      "Epoch 283 Learning rate 0.0001 Error -3.02849522792254\n",
      "Epoch 284 Learning rate 0.0001 Error -3.0145456280822365\n",
      "Epoch 285 Learning rate 0.0001 Error -3.000662281180025\n",
      "Epoch 286 Learning rate 0.0001 Error -2.9868447393491273\n",
      "Epoch 287 Learning rate 0.0001 Error -2.973092558939088\n",
      "Epoch 288 Learning rate 0.0001 Error -2.9594053004653356\n",
      "Epoch 289 Learning rate 0.0001 Error -2.945782528559474\n",
      "Epoch 290 Learning rate 0.0001 Error -2.932223811920289\n",
      "Epoch 291 Learning rate 0.0001 Error -2.9187287232654633\n",
      "Epoch 292 Learning rate 0.0001 Error -2.905296839284009\n",
      "Epoch 293 Learning rate 0.0001 Error -2.8919277405893404\n",
      "Epoch 294 Learning rate 0.0001 Error -2.8786210116730646\n",
      "Epoch 295 Learning rate 0.0001 Error -2.865376240859405\n",
      "Epoch 296 Learning rate 0.0001 Error -2.852193020260279\n",
      "Epoch 297 Learning rate 0.0001 Error -2.839070945731029\n",
      "Epoch 298 Learning rate 0.0001 Error -2.8260096168267506\n",
      "Epoch 299 Learning rate 0.0001 Error -2.813008636759285\n",
      "Epoch 300 Learning rate 0.0001 Error -2.800067612354777\n",
      "Epoch 301 Learning rate 0.0001 Error -2.787186154011852\n",
      "Epoch 302 Learning rate 0.0001 Error -2.7743638756603763\n",
      "Epoch 303 Learning rate 0.0001 Error -2.7616003947207894\n",
      "Epoch 304 Learning rate 0.0001 Error -2.748895332064018\n",
      "Epoch 305 Learning rate 0.0001 Error -2.7362483119719427\n",
      "Epoch 306 Learning rate 0.0001 Error -2.723658962098396\n",
      "Epoch 307 Learning rate 0.0001 Error -2.711126913430732\n",
      "Epoch 308 Learning rate 0.0001 Error -2.6986518002519215\n",
      "Epoch 309 Learning rate 0.0001 Error -2.6862332601031547\n",
      "Epoch 310 Learning rate 0.0001 Error -2.673870933746966\n",
      "Epoch 311 Learning rate 0.0001 Error -2.6615644651308803\n",
      "Epoch 312 Learning rate 0.0001 Error -2.6493135013515277\n",
      "Epoch 313 Learning rate 0.0001 Error -2.637117692619275\n",
      "Epoch 314 Learning rate 0.0001 Error -2.6249766922233406\n",
      "Epoch 315 Learning rate 0.0001 Error -2.61289015649737\n",
      "Epoch 316 Learning rate 0.0001 Error -2.6008577447854746\n",
      "Epoch 317 Learning rate 0.0001 Error -2.588879119408735\n",
      "Epoch 318 Learning rate 0.0001 Error -2.57695394563219\n",
      "Epoch 319 Learning rate 0.0001 Error -2.565081891632218\n",
      "Epoch 320 Learning rate 0.0001 Error -2.5532626284643873\n",
      "Epoch 321 Learning rate 0.0001 Error -2.541495830031731\n",
      "Epoch 322 Learning rate 0.0001 Error -2.5297811730534505\n",
      "Epoch 323 Learning rate 0.0001 Error -2.518118337034033\n",
      "Epoch 324 Learning rate 0.0001 Error -2.506507004232781\n",
      "Epoch 325 Learning rate 0.0001 Error -2.4949468596337305\n",
      "Epoch 326 Learning rate 0.0001 Error -2.483437590916017\n",
      "Epoch 327 Learning rate 0.0001 Error -2.4719788884245775\n",
      "Epoch 328 Learning rate 0.0001 Error -2.460570445141262\n",
      "Epoch 329 Learning rate 0.0001 Error -2.44921195665634\n",
      "Epoch 330 Learning rate 0.0001 Error -2.437903121140354\n",
      "Epoch 331 Learning rate 0.0001 Error -2.4266436393163633\n",
      "Epoch 332 Learning rate 0.0001 Error -2.4154332144325337\n",
      "Epoch 333 Learning rate 0.0001 Error -2.404271552235103\n",
      "Epoch 334 Learning rate 0.0001 Error -2.393158360941684\n",
      "Epoch 335 Learning rate 0.0001 Error -2.382093351214918\n",
      "Epoch 336 Learning rate 0.0001 Error -2.371076236136466\n",
      "Epoch 337 Learning rate 0.0001 Error -2.360106731181354\n",
      "Epoch 338 Learning rate 0.0001 Error -2.349184554192629\n",
      "Epoch 339 Learning rate 0.0001 Error -2.3383094253563375\n",
      "Epoch 340 Learning rate 0.0001 Error -2.3274810671768478\n",
      "Epoch 341 Learning rate 0.0001 Error -2.3166992044524726\n",
      "Epoch 342 Learning rate 0.0001 Error -2.3059635642514094\n",
      "Epoch 343 Learning rate 0.0001 Error -2.2952738758879807\n",
      "Epoch 344 Learning rate 0.0001 Error -2.284629870899179\n",
      "Epoch 345 Learning rate 0.0001 Error -2.2740312830215146\n",
      "Epoch 346 Learning rate 0.0001 Error -2.263477848168152\n",
      "Epoch 347 Learning rate 0.0001 Error -2.252969304406331\n",
      "Epoch 348 Learning rate 0.0001 Error -2.242505391935088\n",
      "Epoch 349 Learning rate 0.0001 Error -2.2320858530632335\n",
      "Epoch 350 Learning rate 0.0001 Error -2.2217104321876335\n",
      "Epoch 351 Learning rate 0.0001 Error -2.211378875771744\n",
      "Epoch 352 Learning rate 0.0001 Error -2.2010909323244148\n",
      "Epoch 353 Learning rate 0.0001 Error -2.190846352378961\n",
      "Epoch 354 Learning rate 0.0001 Error -2.180644888472496\n",
      "Epoch 355 Learning rate 0.0001 Error -2.170486295125529\n",
      "Epoch 356 Learning rate 0.0001 Error -2.160370328821799\n",
      "Epoch 357 Learning rate 0.0001 Error -2.150296747988362\n",
      "Epoch 358 Learning rate 0.0001 Error -2.1402653129759344\n",
      "Epoch 359 Learning rate 0.0001 Error -2.130275786039456\n",
      "Epoch 360 Learning rate 0.0001 Error -2.120327931318939\n",
      "Epoch 361 Learning rate 0.0001 Error -2.110421514820486\n",
      "Epoch 362 Learning rate 0.0001 Error -2.1005563043975837\n",
      "Epoch 363 Learning rate 0.0001 Error -2.090732069732621\n",
      "Epoch 364 Learning rate 0.0001 Error -2.0809485823186202\n",
      "Epoch 365 Learning rate 0.0001 Error -2.0712056154411993\n",
      "Epoch 366 Learning rate 0.0001 Error -2.0615029441607367\n",
      "Epoch 367 Learning rate 0.0001 Error -2.05184034529478\n",
      "Epoch 368 Learning rate 0.0001 Error -2.042217597400638\n",
      "Epoch 369 Learning rate 0.0001 Error -2.032634480758194\n",
      "Epoch 370 Learning rate 0.0001 Error -2.023090777352947\n",
      "Epoch 371 Learning rate 0.0001 Error -2.013586270859213\n",
      "Epoch 372 Learning rate 0.0001 Error -2.004120746623559\n",
      "Epoch 373 Learning rate 0.0001 Error -1.9946939916484199\n",
      "Epoch 374 Learning rate 0.0001 Error -1.9853057945759267\n",
      "Epoch 375 Learning rate 0.0001 Error -1.9759559456719176\n",
      "Epoch 376 Learning rate 0.0001 Error -1.966644236810117\n",
      "Epoch 377 Learning rate 0.0001 Error -1.9573704614565477\n",
      "Epoch 378 Learning rate 0.0001 Error -1.9481344146540946\n",
      "Epoch 379 Learning rate 0.0001 Error -1.9389358930072533\n",
      "Epoch 380 Learning rate 0.0001 Error -1.929774694667055\n",
      "Epoch 381 Learning rate 0.0001 Error -1.9206506193161923\n",
      "Epoch 382 Learning rate 0.0001 Error -1.911563468154299\n",
      "Epoch 383 Learning rate 0.0001 Error -1.902513043883407\n",
      "Epoch 384 Learning rate 0.0001 Error -1.8934991506935586\n",
      "Epoch 385 Learning rate 0.0001 Error -1.884521594248612\n",
      "Epoch 386 Learning rate 0.0001 Error -1.8755801816721998\n",
      "Epoch 387 Learning rate 0.0001 Error -1.8666747215338468\n",
      "Epoch 388 Learning rate 0.0001 Error -1.8578050238352475\n",
      "Epoch 389 Learning rate 0.0001 Error -1.8489708999967134\n",
      "Epoch 390 Learning rate 0.0001 Error -1.840172162843757\n",
      "Epoch 391 Learning rate 0.0001 Error -1.8314086265938614\n",
      "Epoch 392 Learning rate 0.0001 Error -1.8226801068433585\n",
      "Epoch 393 Learning rate 0.0001 Error -1.8139864205544964\n",
      "Epoch 394 Learning rate 0.0001 Error -1.8053273860426398\n",
      "Epoch 395 Learning rate 0.0001 Error -1.7967028229636135\n",
      "Epoch 396 Learning rate 0.0001 Error -1.7881125523011834\n",
      "Epoch 397 Learning rate 0.0001 Error -1.7795563963547023\n",
      "Epoch 398 Learning rate 0.0001 Error -1.7710341787268709\n",
      "Epoch 399 Learning rate 0.0001 Error -1.7625457243116587\n",
      "Epoch 400 Learning rate 0.0001 Error -1.7540908592823468\n",
      "Epoch 401 Learning rate 0.0001 Error -1.745669411079704\n",
      "Epoch 402 Learning rate 0.0001 Error -1.7372812084003062\n",
      "Epoch 403 Learning rate 0.0001 Error -1.728926081184982\n",
      "Epoch 404 Learning rate 0.0001 Error -1.7206038606073935\n",
      "Epoch 405 Learning rate 0.0001 Error -1.7123143790627258\n",
      "Epoch 406 Learning rate 0.0001 Error -1.7040574701565276\n",
      "Epoch 407 Learning rate 0.0001 Error -1.6958329686936617\n",
      "Epoch 408 Learning rate 0.0001 Error -1.6876407106673732\n",
      "Epoch 409 Learning rate 0.0001 Error -1.6794805332484988\n",
      "Epoch 410 Learning rate 0.0001 Error -1.6713522747747787\n",
      "Epoch 411 Learning rate 0.0001 Error -1.6632557747402812\n",
      "Epoch 412 Learning rate 0.0001 Error -1.6551908737849703\n",
      "Epoch 413 Learning rate 0.0001 Error -1.6471574136843619\n",
      "Epoch 414 Learning rate 0.0001 Error -1.6391552373393035\n",
      "Epoch 415 Learning rate 0.0001 Error -1.6311841887658627\n",
      "Epoch 416 Learning rate 0.0001 Error -1.6232441130853443\n",
      "Epoch 417 Learning rate 0.0001 Error -1.6153348565143824\n",
      "Epoch 418 Learning rate 0.0001 Error -1.6074562663551692\n",
      "Epoch 419 Learning rate 0.0001 Error -1.59960819098578\n",
      "Epoch 420 Learning rate 0.0001 Error -1.5917904798505977\n",
      "Epoch 421 Learning rate 0.0001 Error -1.5840029834508484\n",
      "Epoch 422 Learning rate 0.0001 Error -1.5762455533352502\n",
      "Epoch 423 Learning rate 0.0001 Error -1.5685180420907359\n",
      "Epoch 424 Learning rate 0.0001 Error -1.5608203033332875\n",
      "Epoch 425 Learning rate 0.0001 Error -1.5531521916988804\n",
      "Epoch 426 Learning rate 0.0001 Error -1.545513562834516\n",
      "Epoch 427 Learning rate 0.0001 Error -1.537904273389345\n",
      "Epoch 428 Learning rate 0.0001 Error -1.5303241810058978\n",
      "Epoch 429 Learning rate 0.0001 Error -1.5227731443113912\n",
      "Epoch 430 Learning rate 0.0001 Error -1.5152510229091414\n",
      "Epoch 431 Learning rate 0.0001 Error -1.5077576773700727\n",
      "Epoch 432 Learning rate 0.0001 Error -1.5002929692242966\n",
      "Epoch 433 Learning rate 0.0001 Error -1.4928567609528032\n",
      "Epoch 434 Learning rate 0.0001 Error -1.4854489159792175\n",
      "Epoch 435 Learning rate 0.0001 Error -1.478069298661663\n",
      "Epoch 436 Learning rate 0.0001 Error -1.470717774284693\n",
      "Epoch 437 Learning rate 0.0001 Error -1.4633942090513212\n",
      "Epoch 438 Learning rate 0.0001 Error -1.4560984700751307\n",
      "Epoch 439 Learning rate 0.0001 Error -1.448830425372464\n",
      "Epoch 440 Learning rate 0.0001 Error -1.4415899438546913\n",
      "Epoch 441 Learning rate 0.0001 Error -1.4343768953205713\n",
      "Epoch 442 Learning rate 0.0001 Error -1.4271911504486763\n",
      "Epoch 443 Learning rate 0.0001 Error -1.4200325807899064\n",
      "Epoch 444 Learning rate 0.0001 Error -1.4129010587600894\n",
      "Epoch 445 Learning rate 0.0001 Error -1.4057964576326167\n",
      "Epoch 446 Learning rate 0.0001 Error -1.3987186515312175\n",
      "Epoch 447 Learning rate 0.0001 Error -1.391667515422761\n",
      "Epoch 448 Learning rate 0.0001 Error -1.38464292511014\n",
      "Epoch 449 Learning rate 0.0001 Error -1.3776447572252457\n",
      "Epoch 450 Learning rate 0.0001 Error -1.3706728892219973\n",
      "Epoch 451 Learning rate 0.0001 Error -1.3637271993694466\n",
      "Epoch 452 Learning rate 0.0001 Error -1.3568075667449615\n",
      "Epoch 453 Learning rate 0.0001 Error -1.3499138712274625\n",
      "Epoch 454 Learning rate 0.0001 Error -1.3430459934907453\n",
      "Epoch 455 Learning rate 0.0001 Error -1.3362038149968676\n",
      "Epoch 456 Learning rate 0.0001 Error -1.3293872179895896\n",
      "Epoch 457 Learning rate 0.0001 Error -1.3225960854878944\n",
      "Epoch 458 Learning rate 0.0001 Error -1.3158303012795751\n",
      "Epoch 459 Learning rate 0.0001 Error -1.3090897499148695\n",
      "Epoch 460 Learning rate 0.0001 Error -1.302374316700184\n",
      "Epoch 461 Learning rate 0.0001 Error -1.2956838876918604\n",
      "Epoch 462 Learning rate 0.0001 Error -1.2890183496900152\n",
      "Epoch 463 Learning rate 0.0001 Error -1.2823775902324306\n",
      "Epoch 464 Learning rate 0.0001 Error -1.2757614975885243\n",
      "Epoch 465 Learning rate 0.0001 Error -1.2691699607533606\n",
      "Epoch 466 Learning rate 0.0001 Error -1.2626028694417282\n",
      "Epoch 467 Learning rate 0.0001 Error -1.2560601140822887\n",
      "Epoch 468 Learning rate 0.0001 Error -1.2495415858117618\n",
      "Epoch 469 Learning rate 0.0001 Error -1.2430471764691786\n",
      "Epoch 470 Learning rate 0.0001 Error -1.236576778590202\n",
      "Epoch 471 Learning rate 0.0001 Error -1.2301302854014806\n",
      "Epoch 472 Learning rate 0.0001 Error -1.2237075908150765\n",
      "Epoch 473 Learning rate 0.0001 Error -1.2173085894229407\n",
      "Epoch 474 Learning rate 0.0001 Error -1.210933176491445\n",
      "Epoch 475 Learning rate 0.0001 Error -1.2045812479559737\n",
      "Epoch 476 Learning rate 0.0001 Error -1.1982527004155379\n",
      "Epoch 477 Learning rate 0.0001 Error -1.191947431127491\n",
      "Epoch 478 Learning rate 0.0001 Error -1.1856653380022544\n",
      "Epoch 479 Learning rate 0.0001 Error -1.1794063195981235\n",
      "Epoch 480 Learning rate 0.0001 Error -1.1731702751160937\n",
      "Epoch 481 Learning rate 0.0001 Error -1.1669571043947637\n",
      "Epoch 482 Learning rate 0.0001 Error -1.1607667079052821\n",
      "Epoch 483 Learning rate 0.0001 Error -1.1545989867463362\n",
      "Epoch 484 Learning rate 0.0001 Error -1.1484538426391793\n",
      "Epoch 485 Learning rate 0.0001 Error -1.1423311779227352\n",
      "Epoch 486 Learning rate 0.0001 Error -1.136230895548724\n",
      "Epoch 487 Learning rate 0.0001 Error -1.130152899076847\n",
      "Epoch 488 Learning rate 0.0001 Error -1.1240970926700125\n",
      "Epoch 489 Learning rate 0.0001 Error -1.118063381089613\n",
      "Epoch 490 Learning rate 0.0001 Error -1.1120516696908374\n",
      "Epoch 491 Learning rate 0.0001 Error -1.1060618644180311\n",
      "Epoch 492 Learning rate 0.0001 Error -1.1000938718001212\n",
      "Epoch 493 Learning rate 0.0001 Error -1.0941475989460439\n",
      "Epoch 494 Learning rate 0.0001 Error -1.0882229535402557\n",
      "Epoch 495 Learning rate 0.0001 Error -1.0823198438382604\n",
      "Epoch 496 Learning rate 0.0001 Error -1.076438178662189\n",
      "Epoch 497 Learning rate 0.0001 Error -1.0705778673964148\n",
      "Epoch 498 Learning rate 0.0001 Error -1.0647388199832437\n",
      "Epoch 499 Learning rate 0.0001 Error -1.0589209469185694\n",
      "Epoch 500 Learning rate 0.0001 Error -1.0531241592476563\n",
      "Epoch 501 Learning rate 0.0001 Error -1.0473483685608957\n",
      "Epoch 502 Learning rate 0.0001 Error -1.041593486989644\n",
      "Epoch 503 Learning rate 0.0001 Error -1.0358594272020805\n",
      "Epoch 504 Learning rate 0.0001 Error -1.0301461023991\n",
      "Epoch 505 Learning rate 0.0001 Error -1.024453426310257\n",
      "Epoch 506 Learning rate 0.0001 Error -1.0187813131897376\n",
      "Epoch 507 Learning rate 0.0001 Error -1.0131296778123713\n",
      "Epoch 508 Learning rate 0.0001 Error -1.0074984354696874\n",
      "Epoch 509 Learning rate 0.0001 Error -1.0018875019659856\n",
      "Epoch 510 Learning rate 0.0001 Error -0.9962967936144762\n",
      "Epoch 511 Learning rate 0.0001 Error -0.9907262272334337\n",
      "Epoch 512 Learning rate 0.0001 Error -0.9851757201423784\n",
      "Epoch 513 Learning rate 0.0001 Error -0.9796451901583217\n",
      "Epoch 514 Learning rate 0.0001 Error -0.9741345555920061\n",
      "Epoch 515 Learning rate 0.0001 Error -0.9686437352442248\n",
      "Epoch 516 Learning rate 0.0001 Error -0.963172648402139\n",
      "Epoch 517 Learning rate 0.0001 Error -0.9577212148356471\n",
      "Epoch 518 Learning rate 0.0001 Error -0.952289354793778\n",
      "Epoch 519 Learning rate 0.0001 Error -0.9468769890011254\n",
      "Epoch 520 Learning rate 0.0001 Error -0.9414840386543111\n",
      "Epoch 521 Learning rate 0.0001 Error -0.9361104254184723\n",
      "Epoch 522 Learning rate 0.0001 Error -0.9307560714237925\n",
      "Epoch 523 Learning rate 0.0001 Error -0.9254208992620581\n",
      "Epoch 524 Learning rate 0.0001 Error -0.9201048319832443\n",
      "Epoch 525 Learning rate 0.0001 Error -0.91480779309214\n",
      "Epoch 526 Learning rate 0.0001 Error -0.9095297065449857\n",
      "Epoch 527 Learning rate 0.0001 Error -0.9042704967461698\n",
      "Epoch 528 Learning rate 0.0001 Error -0.8990300885449284\n",
      "Epoch 529 Learning rate 0.0001 Error -0.8938084072320777\n",
      "Epoch 530 Learning rate 0.0001 Error -0.888605378536791\n",
      "Epoch 531 Learning rate 0.0001 Error -0.8834209286233909\n",
      "Epoch 532 Learning rate 0.0001 Error -0.8782549840881775\n",
      "Epoch 533 Learning rate 0.0001 Error -0.8731074719562806\n",
      "Epoch 534 Learning rate 0.0001 Error -0.8679783196785349\n",
      "Epoch 535 Learning rate 0.0001 Error -0.8628674551283952\n",
      "Epoch 536 Learning rate 0.0001 Error -0.8577748065988718\n",
      "Epoch 537 Learning rate 0.0001 Error -0.8527003027994929\n",
      "Epoch 538 Learning rate 0.0001 Error -0.8476438728532841\n",
      "Epoch 539 Learning rate 0.0001 Error -0.8426054462938138\n",
      "Epoch 540 Learning rate 0.0001 Error -0.8375849530621932\n",
      "Epoch 541 Learning rate 0.0001 Error -0.8325823235041788\n",
      "Epoch 542 Learning rate 0.0001 Error -0.8275974883672527\n",
      "Epoch 543 Learning rate 0.0001 Error -0.822630378797741\n",
      "Epoch 544 Learning rate 0.0001 Error -0.8176809263379663\n",
      "Epoch 545 Learning rate 0.0001 Error -0.8127490629234013\n",
      "Epoch 546 Learning rate 0.0001 Error -0.8078347208798852\n",
      "Epoch 547 Learning rate 0.0001 Error -0.8029378329208134\n",
      "Epoch 548 Learning rate 0.0001 Error -0.7980583321444074\n",
      "Epoch 549 Learning rate 0.0001 Error -0.7931961520309703\n",
      "Epoch 550 Learning rate 0.0001 Error -0.7883512264401643\n",
      "Epoch 551 Learning rate 0.0001 Error -0.7835234896083476\n",
      "Epoch 552 Learning rate 0.0001 Error -0.7787128761458932\n",
      "Epoch 553 Learning rate 0.0001 Error -0.7739193210345537\n",
      "Epoch 554 Learning rate 0.0001 Error -0.7691427596248362\n",
      "Epoch 555 Learning rate 0.0001 Error -0.7643831276334153\n",
      "Epoch 556 Learning rate 0.0001 Error -0.7596403611405687\n",
      "Epoch 557 Learning rate 0.0001 Error -0.754914396587598\n",
      "Epoch 558 Learning rate 0.0001 Error -0.7502051707743215\n",
      "Epoch 559 Learning rate 0.0001 Error -0.7455126208565537\n",
      "Epoch 560 Learning rate 0.0001 Error -0.7408366843436356\n",
      "Epoch 561 Learning rate 0.0001 Error -0.736177299095951\n",
      "Epoch 562 Learning rate 0.0001 Error -0.7315344033224777\n",
      "Epoch 563 Learning rate 0.0001 Error -0.7269079355783887\n",
      "Epoch 564 Learning rate 0.0001 Error -0.722297834762627\n",
      "Epoch 565 Learning rate 0.0001 Error -0.7177040401155158\n",
      "Epoch 566 Learning rate 0.0001 Error -0.7131264912164252\n",
      "Epoch 567 Learning rate 0.0001 Error -0.7085651279814023\n",
      "Epoch 568 Learning rate 0.0001 Error -0.7040198906608535\n",
      "Epoch 569 Learning rate 0.0001 Error -0.6994907198372452\n",
      "Epoch 570 Learning rate 0.0001 Error -0.694977556422806\n",
      "Epoch 571 Learning rate 0.0001 Error -0.6904803416572842\n",
      "Epoch 572 Learning rate 0.0001 Error -0.6859990171056838\n",
      "Epoch 573 Learning rate 0.0001 Error -0.6815335246560277\n",
      "Epoch 574 Learning rate 0.0001 Error -0.6770838065171635\n",
      "Epoch 575 Learning rate 0.0001 Error -0.6726498052165737\n",
      "Epoch 576 Learning rate 0.0001 Error -0.6682314635981863\n",
      "Epoch 577 Learning rate 0.0001 Error -0.6638287248202311\n",
      "Epoch 578 Learning rate 0.0001 Error -0.659441532353102\n",
      "Epoch 579 Learning rate 0.0001 Error -0.655069829977233\n",
      "Epoch 580 Learning rate 0.0001 Error -0.6507135617809902\n",
      "Epoch 581 Learning rate 0.0001 Error -0.6463726721586033\n",
      "Epoch 582 Learning rate 0.0001 Error -0.6420471058080844\n",
      "Epoch 583 Learning rate 0.0001 Error -0.6377368077291795\n",
      "Epoch 584 Learning rate 0.0001 Error -0.6334417232213309\n",
      "Epoch 585 Learning rate 0.0001 Error -0.6291617978816689\n",
      "Epoch 586 Learning rate 0.0001 Error -0.6248969776029948\n",
      "Epoch 587 Learning rate 0.0001 Error -0.6206472085718158\n",
      "Epoch 588 Learning rate 0.0001 Error -0.6164124372663642\n",
      "Epoch 589 Learning rate 0.0001 Error -0.6121926104546416\n",
      "Epoch 590 Learning rate 0.0001 Error -0.6079876751924886\n",
      "Epoch 591 Learning rate 0.0001 Error -0.6037975788216605\n",
      "Epoch 592 Learning rate 0.0001 Error -0.5996222689679265\n",
      "Epoch 593 Learning rate 0.0001 Error -0.59546169353917\n",
      "Epoch 594 Learning rate 0.0001 Error -0.5913158007235209\n",
      "Epoch 595 Learning rate 0.0001 Error -0.5871845389874932\n",
      "Epoch 596 Learning rate 0.0001 Error -0.5830678570741401\n",
      "Epoch 597 Learning rate 0.0001 Error -0.5789657040012209\n",
      "Epoch 598 Learning rate 0.0001 Error -0.5748780290593941\n",
      "Epoch 599 Learning rate 0.0001 Error -0.5708047818103993\n",
      "Epoch 600 Learning rate 0.0001 Error -0.5667459120852909\n",
      "Epoch 601 Learning rate 0.0001 Error -0.5627013699826485\n",
      "Epoch 602 Learning rate 0.0001 Error -0.5586711058668337\n",
      "Epoch 603 Learning rate 0.0001 Error -0.5546550703662293\n",
      "Epoch 604 Learning rate 0.0001 Error -0.5506532143715244\n",
      "Epoch 605 Learning rate 0.0001 Error -0.5466654890339842\n",
      "Epoch 606 Learning rate 0.0001 Error -0.5426918457637604\n",
      "Epoch 607 Learning rate 0.0001 Error -0.5387322362281829\n",
      "Epoch 608 Learning rate 0.0001 Error -0.5347866123501038\n",
      "Epoch 609 Learning rate 0.0001 Error -0.5308549263062133\n",
      "Epoch 610 Learning rate 0.0001 Error -0.5269371305254148\n",
      "Epoch 611 Learning rate 0.0001 Error -0.52303317768716\n",
      "Epoch 612 Learning rate 0.0001 Error -0.5191430207198504\n",
      "Epoch 613 Learning rate 0.0001 Error -0.5152666127992076\n",
      "Epoch 614 Learning rate 0.0001 Error -0.5114039073466753\n",
      "Epoch 615 Learning rate 0.0001 Error -0.5075548580278447\n",
      "Epoch 616 Learning rate 0.0001 Error -0.5037194187508703\n",
      "Epoch 617 Learning rate 0.0001 Error -0.4998975436649076\n",
      "Epoch 618 Learning rate 0.0001 Error -0.49608918715856776\n",
      "Epoch 619 Learning rate 0.0001 Error -0.49229430385838313\n",
      "Epoch 620 Learning rate 0.0001 Error -0.48851284862727085\n",
      "Epoch 621 Learning rate 0.0001 Error -0.4847447765630266\n",
      "Epoch 622 Learning rate 0.0001 Error -0.4809900429968188\n",
      "Epoch 623 Learning rate 0.0001 Error -0.47724860349169984\n",
      "Epoch 624 Learning rate 0.0001 Error -0.47352041384112387\n",
      "Epoch 625 Learning rate 0.0001 Error -0.46980543006748365\n",
      "Epoch 626 Learning rate 0.0001 Error -0.4661036084206539\n",
      "Epoch 627 Learning rate 0.0001 Error -0.46241490537653496\n",
      "Epoch 628 Learning rate 0.0001 Error -0.4587392776356327\n",
      "Epoch 629 Learning rate 0.0001 Error -0.4550766821216319\n",
      "Epoch 630 Learning rate 0.0001 Error -0.4514270759799722\n",
      "Epoch 631 Learning rate 0.0001 Error -0.44779041657645713\n",
      "Epoch 632 Learning rate 0.0001 Error -0.4441666614958669\n",
      "Epoch 633 Learning rate 0.0001 Error -0.44055576854056155\n",
      "Epoch 634 Learning rate 0.0001 Error -0.436957695729127\n",
      "Epoch 635 Learning rate 0.0001 Error -0.4333724012950142\n",
      "Epoch 636 Learning rate 0.0001 Error -0.42979984368517177\n",
      "Epoch 637 Learning rate 0.0001 Error -0.426239981558725\n",
      "Epoch 638 Learning rate 0.0001 Error -0.4226927737856456\n",
      "Epoch 639 Learning rate 0.0001 Error -0.419158179445426\n",
      "Epoch 640 Learning rate 0.0001 Error -0.41563615782577235\n",
      "Epoch 641 Learning rate 0.0001 Error -0.41212666842129775\n",
      "Epoch 642 Learning rate 0.0001 Error -0.408629670932253\n",
      "Epoch 643 Learning rate 0.0001 Error -0.4051451252632261\n",
      "Epoch 644 Learning rate 0.0001 Error -0.4016729915218792\n",
      "Epoch 645 Learning rate 0.0001 Error -0.39821323001769204\n",
      "Epoch 646 Learning rate 0.0001 Error -0.39476580126070204\n",
      "Epoch 647 Learning rate 0.0001 Error -0.3913306659602721\n",
      "Epoch 648 Learning rate 0.0001 Error -0.3879077850238398\n",
      "Epoch 649 Learning rate 0.0001 Error -0.3844971195557184\n",
      "Epoch 650 Learning rate 0.0001 Error -0.38109863085586493\n",
      "Epoch 651 Learning rate 0.0001 Error -0.3777122804186773\n",
      "Epoch 652 Learning rate 0.0001 Error -0.37433802993180065\n",
      "Epoch 653 Learning rate 0.0001 Error -0.3709758412749412\n",
      "Epoch 654 Learning rate 0.0001 Error -0.36762567651867284\n",
      "Epoch 655 Learning rate 0.0001 Error -0.3642874979232824\n",
      "Epoch 656 Learning rate 0.0001 Error -0.3609612679376102\n",
      "Epoch 657 Learning rate 0.0001 Error -0.3576469491978791\n",
      "Epoch 658 Learning rate 0.0001 Error -0.35434450452656274\n",
      "Epoch 659 Learning rate 0.0001 Error -0.35105389693124367\n",
      "Epoch 660 Learning rate 0.0001 Error -0.34777508960348713\n",
      "Epoch 661 Learning rate 0.0001 Error -0.344508045917721\n",
      "Epoch 662 Learning rate 0.0001 Error -0.34125272943012697\n",
      "Epoch 663 Learning rate 0.0001 Error -0.3380091038775228\n",
      "Epoch 664 Learning rate 0.0001 Error -0.3347771331762921\n",
      "Epoch 665 Learning rate 0.0001 Error -0.3315567814212619\n",
      "Epoch 666 Learning rate 0.0001 Error -0.3283480128846584\n",
      "Epoch 667 Learning rate 0.0001 Error -0.32515079201500663\n",
      "Epoch 668 Learning rate 0.0001 Error -0.32196508343607877\n",
      "Epoch 669 Learning rate 0.0001 Error -0.3187908519458402\n",
      "Epoch 670 Learning rate 0.0001 Error -0.31562806251539754\n",
      "Epoch 671 Learning rate 0.0001 Error -0.31247668028794917\n",
      "Epoch 672 Learning rate 0.0001 Error -0.3093366705777598\n",
      "Epoch 673 Learning rate 0.0001 Error -0.3062079988691345\n",
      "Epoch 674 Learning rate 0.0001 Error -0.30309063081539034\n",
      "Epoch 675 Learning rate 0.0001 Error -0.29998453223785126\n",
      "Epoch 676 Learning rate 0.0001 Error -0.2968896691248408\n",
      "Epoch 677 Learning rate 0.0001 Error -0.2938060076306819\n",
      "Epoch 678 Learning rate 0.0001 Error -0.29073351407471315\n",
      "Epoch 679 Learning rate 0.0001 Error -0.28767215494030496\n",
      "Epoch 680 Learning rate 0.0001 Error -0.2846218968738783\n",
      "Epoch 681 Learning rate 0.0001 Error -0.28158270668392904\n",
      "Epoch 682 Learning rate 0.0001 Error -0.2785545513400699\n",
      "Epoch 683 Learning rate 0.0001 Error -0.2755373979720901\n",
      "Epoch 684 Learning rate 0.0001 Error -0.27253121386897994\n",
      "Epoch 685 Learning rate 0.0001 Error -0.26953596647800937\n",
      "Epoch 686 Learning rate 0.0001 Error -0.26655162340377436\n",
      "Epoch 687 Learning rate 0.0001 Error -0.26357815240728033\n",
      "Epoch 688 Learning rate 0.0001 Error -0.26061552140501565\n",
      "Epoch 689 Learning rate 0.0001 Error -0.25766369846803394\n",
      "Epoch 690 Learning rate 0.0001 Error -0.2547226518210376\n",
      "Epoch 691 Learning rate 0.0001 Error -0.251792349841486\n",
      "Epoch 692 Learning rate 0.0001 Error -0.2488727610587027\n",
      "Epoch 693 Learning rate 0.0001 Error -0.2459638541529603\n",
      "Epoch 694 Learning rate 0.0001 Error -0.24306559795460903\n",
      "Epoch 695 Learning rate 0.0001 Error -0.24017796144321046\n",
      "Epoch 696 Learning rate 0.0001 Error -0.23730091374664664\n",
      "Epoch 697 Learning rate 0.0001 Error -0.23443442414025406\n",
      "Epoch 698 Learning rate 0.0001 Error -0.23157846204597599\n",
      "Epoch 699 Learning rate 0.0001 Error -0.22873299703149486\n",
      "Epoch 700 Learning rate 0.0001 Error -0.22589799880939188\n",
      "Epoch 701 Learning rate 0.0001 Error -0.22307343723630013\n",
      "Epoch 702 Learning rate 0.0001 Error -0.22025928231206393\n",
      "Epoch 703 Learning rate 0.0001 Error -0.217455504178918\n",
      "Epoch 704 Learning rate 0.0001 Error -0.21466207312065555\n",
      "Epoch 705 Learning rate 0.0001 Error -0.21187895956179537\n",
      "Epoch 706 Learning rate 0.0001 Error -0.2091061340667909\n",
      "Epoch 707 Learning rate 0.0001 Error -0.20634356733920223\n",
      "Epoch 708 Learning rate 0.0001 Error -0.20359123022089787\n",
      "Epoch 709 Learning rate 0.0001 Error -0.20084909369126835\n",
      "Epoch 710 Learning rate 0.0001 Error -0.19811712886641553\n",
      "Epoch 711 Learning rate 0.0001 Error -0.19539530699838203\n",
      "Epoch 712 Learning rate 0.0001 Error -0.19268359947435376\n",
      "Epoch 713 Learning rate 0.0001 Error -0.18998197781589166\n",
      "Epoch 714 Learning rate 0.0001 Error -0.18729041367816124\n",
      "Epoch 715 Learning rate 0.0001 Error -0.184608878849167\n",
      "Epoch 716 Learning rate 0.0001 Error -0.18193734524897676\n",
      "Epoch 717 Learning rate 0.0001 Error -0.17927578492899232\n",
      "Epoch 718 Learning rate 0.0001 Error -0.17662417007117392\n",
      "Epoch 719 Learning rate 0.0001 Error -0.17398247298730907\n",
      "Epoch 720 Learning rate 0.0001 Error -0.17135066611826588\n",
      "Epoch 721 Learning rate 0.0001 Error -0.16872872203326317\n",
      "Epoch 722 Learning rate 0.0001 Error -0.1661166134291262\n",
      "Epoch 723 Learning rate 0.0001 Error -0.16351431312957743\n",
      "Epoch 724 Learning rate 0.0001 Error -0.16092179408449614\n",
      "Epoch 725 Learning rate 0.0001 Error -0.1583390293692175\n",
      "Epoch 726 Learning rate 0.0001 Error -0.15576599218382847\n",
      "Epoch 727 Learning rate 0.0001 Error -0.15320265585243076\n",
      "Epoch 728 Learning rate 0.0001 Error -0.15064899382246733\n",
      "Epoch 729 Learning rate 0.0001 Error -0.14810497966401504\n",
      "Epoch 730 Learning rate 0.0001 Error -0.14557058706907977\n",
      "Epoch 731 Learning rate 0.0001 Error -0.14304578985092709\n",
      "Epoch 732 Learning rate 0.0001 Error -0.14053056194339208\n",
      "Epoch 733 Learning rate 0.0001 Error -0.1380248774001871\n",
      "Epoch 734 Learning rate 0.0001 Error -0.13552871039424752\n",
      "Epoch 735 Learning rate 0.0001 Error -0.1330420352170425\n",
      "Epoch 736 Learning rate 0.0001 Error -0.13056482627792787\n",
      "Epoch 737 Learning rate 0.0001 Error -0.12809705810345917\n",
      "Epoch 738 Learning rate 0.0001 Error -0.1256387053367578\n",
      "Epoch 739 Learning rate 0.0001 Error -0.12318974273684968\n",
      "Epoch 740 Learning rate 0.0001 Error -0.1207501451780133\n",
      "Epoch 741 Learning rate 0.0001 Error -0.11831988764913492\n",
      "Epoch 742 Learning rate 0.0001 Error -0.11589894525306643\n",
      "Epoch 743 Learning rate 0.0001 Error -0.11348729320600226\n",
      "Epoch 744 Learning rate 0.0001 Error -0.11108490683682792\n",
      "Epoch 745 Learning rate 0.0001 Error -0.1086917615865054\n",
      "Epoch 746 Learning rate 0.0001 Error -0.10630783300744528\n",
      "Epoch 747 Learning rate 0.0001 Error -0.10393309676287998\n",
      "Epoch 748 Learning rate 0.0001 Error -0.10156752862625851\n",
      "Epoch 749 Learning rate 0.0001 Error -0.09921110448062753\n",
      "Epoch 750 Learning rate 0.0001 Error -0.09686380031802833\n",
      "Epoch 751 Learning rate 0.0001 Error -0.09452559223888346\n",
      "Epoch 752 Learning rate 0.0001 Error -0.09219645645140473\n",
      "Epoch 753 Learning rate 0.0001 Error -0.08987636927098797\n",
      "Epoch 754 Learning rate 0.0001 Error -0.08756530711962962\n",
      "Epoch 755 Learning rate 0.0001 Error -0.08526324652533779\n",
      "Epoch 756 Learning rate 0.0001 Error -0.08297016412153235\n",
      "Epoch 757 Learning rate 0.0001 Error -0.08068603664648466\n",
      "Epoch 758 Learning rate 0.0001 Error -0.0784108409427201\n",
      "Epoch 759 Learning rate 0.0001 Error -0.07614455395645181\n",
      "Epoch 760 Learning rate 0.0001 Error -0.07388715273701263\n",
      "Epoch 761 Learning rate 0.0001 Error -0.0716386144362896\n",
      "Epoch 762 Learning rate 0.0001 Error -0.06939891630815087\n",
      "Epoch 763 Learning rate 0.0001 Error -0.06716803570789043\n",
      "Epoch 764 Learning rate 0.0001 Error -0.06494595009167892\n",
      "Epoch 765 Learning rate 0.0001 Error -0.06273263701599363\n",
      "Epoch 766 Learning rate 0.0001 Error -0.06052807413709438\n",
      "Epoch 767 Learning rate 0.0001 Error -0.0583322392104465\n",
      "Epoch 768 Learning rate 0.0001 Error -0.05614511009020573\n",
      "Epoch 769 Learning rate 0.0001 Error -0.053966664728667624\n",
      "Epoch 770 Learning rate 0.0001 Error -0.051796881175723675\n",
      "Epoch 771 Learning rate 0.0001 Error -0.04963573757834283\n",
      "Epoch 772 Learning rate 0.0001 Error -0.04748321218003315\n",
      "Epoch 773 Learning rate 0.0001 Error -0.04533928332033077\n",
      "Epoch 774 Learning rate 0.0001 Error -0.04320392943424689\n",
      "Epoch 775 Learning rate 0.0001 Error -0.04107712905177774\n",
      "Epoch 776 Learning rate 0.0001 Error -0.038958860797371875\n",
      "Epoch 777 Learning rate 0.0001 Error -0.03684910338943026\n",
      "Epoch 778 Learning rate 0.0001 Error -0.03474783563978323\n",
      "Epoch 779 Learning rate 0.0001 Error -0.032655036453187236\n",
      "Epoch 780 Learning rate 0.0001 Error -0.030570684826830008\n",
      "Epoch 781 Learning rate 0.0001 Error -0.02849475984981975\n",
      "Epoch 782 Learning rate 0.0001 Error -0.026427240702688426\n",
      "Epoch 783 Learning rate 0.0001 Error -0.024368106656901256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 784 Learning rate 0.0001 Error -0.022317337074362897\n",
      "Epoch 785 Learning rate 0.0001 Error -0.020274911406933716\n",
      "Epoch 786 Learning rate 0.0001 Error -0.01824080919593951\n",
      "Epoch 787 Learning rate 0.0001 Error -0.016215010071690683\n",
      "Epoch 788 Learning rate 0.0001 Error -0.01419749375299828\n",
      "Epoch 789 Learning rate 0.0001 Error -0.012188240046703824\n",
      "Epoch 790 Learning rate 0.0001 Error -0.010187228847199692\n",
      "Epoch 791 Learning rate 0.0001 Error -0.00819444013596149\n",
      "Epoch 792 Learning rate 0.0001 Error -0.0062098539810849784\n",
      "Epoch 793 Learning rate 0.0001 Error -0.004233450536818895\n",
      "Epoch 794 Learning rate 0.0001 Error -0.002265210043083443\n",
      "Epoch 795 Learning rate 0.0001 Error -0.0003051128250423085\n",
      "Epoch 796 Learning rate 0.0001 Error 0.0016468607073801822\n",
      "Epoch 797 Learning rate 0.0001 Error 0.00359073005993904\n",
      "Epoch 798 Learning rate 0.0001 Error 0.005526514654528358\n",
      "Epoch 799 Learning rate 0.0001 Error 0.007454233829619961\n",
      "Epoch 800 Learning rate 0.0001 Error 0.009373906840717816\n",
      "Epoch 801 Learning rate 0.0001 Error 0.011285552860799464\n",
      "Epoch 802 Learning rate 0.0001 Error 0.013189190980759324\n",
      "Epoch 803 Learning rate 0.0001 Error 0.015084840209840578\n",
      "Epoch 804 Learning rate 0.0001 Error 0.016972519476075254\n",
      "Epoch 805 Learning rate 0.0001 Error 0.018852247626718888\n",
      "Epoch 806 Learning rate 0.0001 Error 0.020724043428676064\n",
      "Epoch 807 Learning rate 0.0001 Error 0.022587925568924416\n",
      "Epoch 808 Learning rate 0.0001 Error 0.024443912654952604\n",
      "Epoch 809 Learning rate 0.0001 Error 0.02629202321518287\n",
      "Epoch 810 Learning rate 0.0001 Error 0.028132275699369158\n",
      "Epoch 811 Learning rate 0.0001 Error 0.029964688479046098\n",
      "Epoch 812 Learning rate 0.0001 Error 0.03178927984792368\n",
      "Epoch 813 Learning rate 0.0001 Error 0.033606068022300706\n",
      "Epoch 814 Learning rate 0.0001 Error 0.03541507114148401\n",
      "Epoch 815 Learning rate 0.0001 Error 0.037216307268201465\n",
      "Epoch 816 Learning rate 0.0001 Error 0.039009794388985664\n",
      "Epoch 817 Learning rate 0.0001 Error 0.04079555041460137\n",
      "Epoch 818 Learning rate 0.0001 Error 0.04257359318042797\n",
      "Epoch 819 Learning rate 0.0001 Error 0.04434394044687662\n",
      "Epoch 820 Learning rate 0.0001 Error 0.04610660989977655\n",
      "Epoch 821 Learning rate 0.0001 Error 0.04786161915075615\n",
      "Epoch 822 Learning rate 0.0001 Error 0.04960898573766148\n",
      "Epoch 823 Learning rate 0.0001 Error 0.051348727124928994\n",
      "Epoch 824 Learning rate 0.0001 Error 0.053080860703972\n",
      "Epoch 825 Learning rate 0.0001 Error 0.0548054037935779\n",
      "Epoch 826 Learning rate 0.0001 Error 0.05652237364027557\n",
      "Epoch 827 Learning rate 0.0001 Error 0.0582317874187247\n",
      "Epoch 828 Learning rate 0.0001 Error 0.05993366223210117\n",
      "Epoch 829 Learning rate 0.0001 Error 0.061628015112462076\n",
      "Epoch 830 Learning rate 0.0001 Error 0.06331486302112221\n",
      "Epoch 831 Learning rate 0.0001 Error 0.06499422284902912\n",
      "Epoch 832 Learning rate 0.0001 Error 0.06666611141712875\n",
      "Epoch 833 Learning rate 0.0001 Error 0.06833054547674222\n",
      "Epoch 834 Learning rate 0.0001 Error 0.0699875417099195\n",
      "Epoch 835 Learning rate 0.0001 Error 0.07163711672980744\n",
      "Epoch 836 Learning rate 0.0001 Error 0.07327928708101772\n",
      "Epoch 837 Learning rate 0.0001 Error 0.07491406923998456\n",
      "Epoch 838 Learning rate 0.0001 Error 0.07654147961531521\n",
      "Epoch 839 Learning rate 0.0001 Error 0.07816153454814734\n",
      "Epoch 840 Learning rate 0.0001 Error 0.07977425031251528\n",
      "Epoch 841 Learning rate 0.0001 Error 0.0813796431156768\n",
      "Epoch 842 Learning rate 0.0001 Error 0.08297772909848278\n",
      "Epoch 843 Learning rate 0.0001 Error 0.08456852433572648\n",
      "Epoch 844 Learning rate 0.0001 Error 0.08615204483647365\n",
      "Epoch 845 Learning rate 0.0001 Error 0.08772830654441288\n",
      "Epoch 846 Learning rate 0.0001 Error 0.08929732533820911\n",
      "Epoch 847 Learning rate 0.0001 Error 0.09085911703182104\n",
      "Epoch 848 Learning rate 0.0001 Error 0.09241369737486027\n",
      "Epoch 849 Learning rate 0.0001 Error 0.09396108205291498\n",
      "Epoch 850 Learning rate 0.0001 Error 0.09550128668788538\n",
      "Epoch 851 Learning rate 0.0001 Error 0.09703432683832092\n",
      "Epoch 852 Learning rate 0.0001 Error 0.09856021799975245\n",
      "Epoch 853 Learning rate 0.0001 Error 0.10007897560500861\n",
      "Epoch 854 Learning rate 0.0001 Error 0.10159061502456102\n",
      "Epoch 855 Learning rate 0.0001 Error 0.10309515156682952\n",
      "Epoch 856 Learning rate 0.0001 Error 0.10459260047851915\n",
      "Epoch 857 Learning rate 0.0001 Error 0.10608297694493357\n",
      "Epoch 858 Learning rate 0.0001 Error 0.10756629609030949\n",
      "Epoch 859 Learning rate 0.0001 Error 0.10904257297811354\n",
      "Epoch 860 Learning rate 0.0001 Error 0.11051182261137682\n",
      "Epoch 861 Learning rate 0.0001 Error 0.11197405993299797\n",
      "Epoch 862 Learning rate 0.0001 Error 0.11342929982605482\n",
      "Epoch 863 Learning rate 0.0001 Error 0.11487755711413161\n",
      "Epoch 864 Learning rate 0.0001 Error 0.1163188465616059\n",
      "Epoch 865 Learning rate 0.0001 Error 0.11775318287397935\n",
      "Epoch 866 Learning rate 0.0001 Error 0.11918058069816051\n",
      "Epoch 867 Learning rate 0.0001 Error 0.1206010546227887\n",
      "Epoch 868 Learning rate 0.0001 Error 0.1220146191785253\n",
      "Epoch 869 Learning rate 0.0001 Error 0.12342128883835607\n",
      "Epoch 870 Learning rate 0.0001 Error 0.12482107801790121\n",
      "Epoch 871 Learning rate 0.0001 Error 0.12621400107569947\n",
      "Epoch 872 Learning rate 0.0001 Error 0.12760007231350812\n",
      "Epoch 873 Learning rate 0.0001 Error 0.12897930597659407\n",
      "Epoch 874 Learning rate 0.0001 Error 0.13035171625404962\n",
      "Epoch 875 Learning rate 0.0001 Error 0.13171731727905578\n",
      "Epoch 876 Learning rate 0.0001 Error 0.1330761231291836\n",
      "Epoch 877 Learning rate 0.0001 Error 0.13442814782667778\n",
      "Epoch 878 Learning rate 0.0001 Error 0.13577340533875637\n",
      "Epoch 879 Learning rate 0.0001 Error 0.13711190957788383\n",
      "Epoch 880 Learning rate 0.0001 Error 0.13844367440205463\n",
      "Epoch 881 Learning rate 0.0001 Error 0.13976871361508358\n",
      "Epoch 882 Learning rate 0.0001 Error 0.14108704096688368\n",
      "Epoch 883 Learning rate 0.0001 Error 0.14239867015374225\n",
      "Epoch 884 Learning rate 0.0001 Error 0.14370361481860067\n",
      "Epoch 885 Learning rate 0.0001 Error 0.14500188855133317\n",
      "Epoch 886 Learning rate 0.0001 Error 0.14629350488901594\n",
      "Epoch 887 Learning rate 0.0001 Error 0.1475784773162031\n",
      "Epoch 888 Learning rate 0.0001 Error 0.14885681926520705\n",
      "Epoch 889 Learning rate 0.0001 Error 0.15012854411635618\n",
      "Epoch 890 Learning rate 0.0001 Error 0.15139366519827152\n",
      "Epoch 891 Learning rate 0.0001 Error 0.15265219578813177\n",
      "Epoch 892 Learning rate 0.0001 Error 0.15390414911193756\n",
      "Epoch 893 Learning rate 0.0001 Error 0.15514953834477796\n",
      "Epoch 894 Learning rate 0.0001 Error 0.15638837661110416\n",
      "Epoch 895 Learning rate 0.0001 Error 0.15762067698497834\n",
      "Epoch 896 Learning rate 0.0001 Error 0.1588464524903367\n",
      "Epoch 897 Learning rate 0.0001 Error 0.16006571610125342\n",
      "Epoch 898 Learning rate 0.0001 Error 0.1612784807421962\n",
      "Epoch 899 Learning rate 0.0001 Error 0.16248475928828565\n",
      "Epoch 900 Learning rate 0.0001 Error 0.16368456456554392\n",
      "Epoch 901 Learning rate 0.0001 Error 0.1648779093511618\n",
      "Epoch 902 Learning rate 0.0001 Error 0.166064806373736\n",
      "Epoch 903 Learning rate 0.0001 Error 0.1672452683135276\n",
      "Epoch 904 Learning rate 0.0001 Error 0.16841930780271347\n",
      "Epoch 905 Learning rate 0.0001 Error 0.16958693742564435\n",
      "Epoch 906 Learning rate 0.0001 Error 0.17074816971906354\n",
      "Epoch 907 Learning rate 0.0001 Error 0.17190301717238832\n",
      "Epoch 908 Learning rate 0.0001 Error 0.1730514922279327\n",
      "Epoch 909 Learning rate 0.0001 Error 0.17419360728116284\n",
      "Epoch 910 Learning rate 0.0001 Error 0.1753293746809278\n",
      "Epoch 911 Learning rate 0.0001 Error 0.17645880672971004\n",
      "Epoch 912 Learning rate 0.0001 Error 0.17758191568386716\n",
      "Epoch 913 Learning rate 0.0001 Error 0.17869871375386165\n",
      "Epoch 914 Learning rate 0.0001 Error 0.17980921310450215\n",
      "Epoch 915 Learning rate 0.0001 Error 0.1809134258551926\n",
      "Epoch 916 Learning rate 0.0001 Error 0.18201136408014484\n",
      "Epoch 917 Learning rate 0.0001 Error 0.18310303980862874\n",
      "Epoch 918 Learning rate 0.0001 Error 0.18418846502520486\n",
      "Epoch 919 Learning rate 0.0001 Error 0.18526765166995013\n",
      "Epoch 920 Learning rate 0.0001 Error 0.18634061163868443\n",
      "Epoch 921 Learning rate 0.0001 Error 0.1874073567832133\n",
      "Epoch 922 Learning rate 0.0001 Error 0.1884678989115533\n",
      "Epoch 923 Learning rate 0.0001 Error 0.1895222497881443\n",
      "Epoch 924 Learning rate 0.0001 Error 0.19057042113409334\n",
      "Epoch 925 Learning rate 0.0001 Error 0.19161242462739753\n",
      "Epoch 926 Learning rate 0.0001 Error 0.19264827190314726\n",
      "Epoch 927 Learning rate 0.0001 Error 0.19367797455377772\n",
      "Epoch 928 Learning rate 0.0001 Error 0.1947015441292757\n",
      "Epoch 929 Learning rate 0.0001 Error 0.1957189921373963\n",
      "Epoch 930 Learning rate 0.0001 Error 0.19673033004389\n",
      "Epoch 931 Learning rate 0.0001 Error 0.1977355692727178\n",
      "Epoch 932 Learning rate 0.0001 Error 0.19873472120627284\n",
      "Epoch 933 Learning rate 0.0001 Error 0.1997277971855866\n",
      "Epoch 934 Learning rate 0.0001 Error 0.2007148085105519\n",
      "Epoch 935 Learning rate 0.0001 Error 0.2016957664401362\n",
      "Epoch 936 Learning rate 0.0001 Error 0.20267068219259665\n",
      "Epoch 937 Learning rate 0.0001 Error 0.20363956694568625\n",
      "Epoch 938 Learning rate 0.0001 Error 0.2046024318368601\n",
      "Epoch 939 Learning rate 0.0001 Error 0.20555928796350242\n",
      "Epoch 940 Learning rate 0.0001 Error 0.20651014638311704\n",
      "Epoch 941 Learning rate 0.0001 Error 0.20745501811354605\n",
      "Epoch 942 Learning rate 0.0001 Error 0.20839391413317476\n",
      "Epoch 943 Learning rate 0.0001 Error 0.20932684538113427\n",
      "Epoch 944 Learning rate 0.0001 Error 0.21025382275750715\n",
      "Epoch 945 Learning rate 0.0001 Error 0.211174857123532\n",
      "Epoch 946 Learning rate 0.0001 Error 0.21208995930181251\n",
      "Epoch 947 Learning rate 0.0001 Error 0.21299914007650578\n",
      "Epoch 948 Learning rate 0.0001 Error 0.21390241019353262\n",
      "Epoch 949 Learning rate 0.0001 Error 0.21479978036077807\n",
      "Epoch 950 Learning rate 0.0001 Error 0.21569126124828336\n",
      "Epoch 951 Learning rate 0.0001 Error 0.21657686348844507\n",
      "Epoch 952 Learning rate 0.0001 Error 0.21745659767621556\n",
      "Epoch 953 Learning rate 0.0001 Error 0.2183304743693053\n",
      "Epoch 954 Learning rate 0.0001 Error 0.21919850408835828\n",
      "Epoch 955 Learning rate 0.0001 Error 0.22006069731716665\n",
      "Epoch 956 Learning rate 0.0001 Error 0.22091706450285714\n",
      "Epoch 957 Learning rate 0.0001 Error 0.22176761605607415\n",
      "Epoch 958 Learning rate 0.0001 Error 0.22261236235117882\n",
      "Epoch 959 Learning rate 0.0001 Error 0.2234513137264471\n",
      "Epoch 960 Learning rate 0.0001 Error 0.2242844804842411\n",
      "Epoch 961 Learning rate 0.0001 Error 0.22511187289121598\n",
      "Epoch 962 Learning rate 0.0001 Error 0.22593350117849026\n",
      "Epoch 963 Learning rate 0.0001 Error 0.2267493755418482\n",
      "Epoch 964 Learning rate 0.0001 Error 0.2275595061419194\n",
      "Epoch 965 Learning rate 0.0001 Error 0.2283639031043605\n",
      "Epoch 966 Learning rate 0.0001 Error 0.2291625765200454\n",
      "Epoch 967 Learning rate 0.0001 Error 0.22995553644523842\n",
      "Epoch 968 Learning rate 0.0001 Error 0.23074279290179\n",
      "Epoch 969 Learning rate 0.0001 Error 0.2315243558773133\n",
      "Epoch 970 Learning rate 0.0001 Error 0.23230023532535748\n",
      "Epoch 971 Learning rate 0.0001 Error 0.23307044116559567\n",
      "Epoch 972 Learning rate 0.0001 Error 0.23383498328400004\n",
      "Epoch 973 Learning rate 0.0001 Error 0.23459387153302397\n",
      "Epoch 974 Learning rate 0.0001 Error 0.2353471157317759\n",
      "Epoch 975 Learning rate 0.0001 Error 0.236094725666197\n",
      "Epoch 976 Learning rate 0.0001 Error 0.23683671108923848\n",
      "Epoch 977 Learning rate 0.0001 Error 0.23757308172103125\n",
      "Epoch 978 Learning rate 0.0001 Error 0.23830384724906006\n",
      "Epoch 979 Learning rate 0.0001 Error 0.2390290173283437\n",
      "Epoch 980 Learning rate 0.0001 Error 0.2397486015816071\n",
      "Epoch 981 Learning rate 0.0001 Error 0.24046260959944743\n",
      "Epoch 982 Learning rate 0.0001 Error 0.24117105094049984\n",
      "Epoch 983 Learning rate 0.0001 Error 0.24187393513162225\n",
      "Epoch 984 Learning rate 0.0001 Error 0.24257127166805703\n",
      "Epoch 985 Learning rate 0.0001 Error 0.24326307001358727\n",
      "Epoch 986 Learning rate 0.0001 Error 0.24394933960072124\n",
      "Epoch 987 Learning rate 0.0001 Error 0.24463008983086176\n",
      "Epoch 988 Learning rate 0.0001 Error 0.2453053300744581\n",
      "Epoch 989 Learning rate 0.0001 Error 0.24597506967117816\n",
      "Epoch 990 Learning rate 0.0001 Error 0.24663931793007277\n",
      "Epoch 991 Learning rate 0.0001 Error 0.24729808412973842\n",
      "Epoch 992 Learning rate 0.0001 Error 0.24795137751848123\n",
      "Epoch 993 Learning rate 0.0001 Error 0.24859920731447566\n",
      "Epoch 994 Learning rate 0.0001 Error 0.2492415827059406\n",
      "Epoch 995 Learning rate 0.0001 Error 0.2498785128512781\n",
      "Epoch 996 Learning rate 0.0001 Error 0.2505100068792433\n",
      "Epoch 997 Learning rate 0.0001 Error 0.2511360738891142\n",
      "Epoch 998 Learning rate 0.0001 Error 0.25175672295083473\n",
      "Epoch 999 Learning rate 0.0001 Error 0.2523719631051792\n",
      "[-0.3470523734852804, 0.19027171505355178, -0.24821588954152454, -0.3470523734852804, 0.13728839423511333, 0.10754046864803057, -0.3470523734852804, 0.004730996861104379, -0.4505897192574122]\n"
     ]
    }
   ],
   "source": [
    "def train_weights(train,learningrate,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction,first_layer = predict(row,weights)\n",
    "            error = row[-1]-prediction\n",
    "            sum_error += error\n",
    "            #First layer\n",
    "            weights[0] = weights[0] + learningrate*error*1\n",
    "            weights[3] = weights[3] + learningrate*error\n",
    "\n",
    "            weights[1] = weights[1] + learningrate*error*row[0]\n",
    "            weights[2] = weights[2] + learningrate*error*row[1]\n",
    "            weights[4] = weights[4] + learningrate*error*row[2]\n",
    "            weights[5] = weights[5] + learningrate*error*row[3]\n",
    "\n",
    "            #Second layer\n",
    "            weights[6] = weights[6] + learningrate*error\n",
    "            weights[7] = weights[7] + learningrate*error*first_layer[0]\n",
    "            weights[8] = weights[8] + learningrate*error*first_layer[1]\n",
    "        if((epoch%100==0) or (last_error != sum_error)):\n",
    "            print(\"Epoch \"+str(epoch) + \" Learning rate \" + str(learningrate) + \" Error \" + str(sum_error))\n",
    "        last_error = sum_error\n",
    "    return weights\n",
    "\n",
    "learningrate = 0.0001 #0.00001\n",
    "epochs = 1000\n",
    "train_weights = train_weights(training_dataset,learningrate,epochs)\n",
    "print(train_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
